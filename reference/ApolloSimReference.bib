
@article{noauthor_notitle_nodate,
}

@misc{winiwarter_virtual_2021,
	title = {Virtual laser scanning with {HELIOS}++: {A} novel take on ray tracing-based simulation of topographic {3D} laser scanning},
	shorttitle = {Virtual laser scanning with {HELIOS}++},
	url = {http://arxiv.org/abs/2101.09154},
	abstract = {Topographic laser scanning is a remote sensing method to create detailed 3D point cloud representations of the Earth's surface. Since data acquisition is expensive, simulations can complement real data given certain premises are available: i) a model of 3D scene and scanner, ii) a model of the beam-scene interaction, simplified to a computationally feasible while physically realistic level, and iii) an application for which simulated data is fit for use. A number of laser scanning simulators for different purposes exist, which we enrich by presenting HELIOS++. HELIOS++ is an open-source simulation framework for terrestrial static, mobile, UAV-based and airborne laser scanning implemented in C++. The HELIOS++ concept provides a flexible solution for the trade-off between physical accuracy (realism) and computational complexity (runtime, memory footprint), as well as ease of use and of configuration. Unique features of HELIOS++ include the availability of Python bindings (pyhelios) for controlling simulations, and a range of model types for 3D scene representation. HELIOS++ further allows the simulation of beam divergence using a subsampling strategy, and is able to create full-waveform outputs as a basis for detailed analysis. As generation and analysis of waveforms can strongly impact runtimes, the user may set the level of detail for the subsampling, or optionally disable full-waveform output altogether. A detailed assessment of computational considerations and a comparison of HELIOS++ to its predecessor, HELIOS, reveal reduced runtimes by up to 83 \%. At the same time, memory requirements are reduced by up to 94 \%, allowing for much larger (i.e. more complex) 3D scenes to be loaded into memory and hence to be virtually acquired by laser scanning simulation.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Winiwarter, Lukas and Pena, Alberto Manuel Esmorís and Weiser, Hannah and Anders, Katharina and Sanchez, Jorge Martínez and Searle, Mark and Höfle, Bernhard},
	month = jan,
	year = {2021},
	note = {Issue: arXiv:2101.09154
arXiv:2101.09154 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{lopez_gpu-accelerated_2022,
	title = {A {GPU}-{Accelerated} {Framework} for {Simulating} {LiDAR} {Scanning}},
	volume = {60},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9751040/},
	doi = {10.1109/TGRS.2022.3165746},
	urldate = {2024-01-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lopez, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	year = {2022},
	pages = {1--18},
}

@misc{manivasagam_lidarsim_2020,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	shorttitle = {{LiDARsim}},
	url = {http://arxiv.org/abs/2006.09348},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {Issue: arXiv:2006.09348
arXiv:2006.09348 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020 (Oral)},
}

@article{kim_simulation_2013,
	title = {Simulation of a {Geiger}-{Mode} {Imaging} {LADAR} {System} for {Performance} {Assessment}},
	volume = {13},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/13/7/8461},
	doi = {10.3390/s130708461},
	language = {en},
	number = {7},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Kim, Seongjoon and Lee, Impyeong and Kwon, Yong},
	month = jul,
	year = {2013},
	note = {Number: 7},
	pages = {8461--8489},
}

@article{wang_dart-lux_2022,
	title = {{DART}-{Lux}: {An} unbiased and rapid {Monte} {Carlo} radiative transfer method for simulating remote sensing images},
	volume = {274},
	issn = {00344257},
	shorttitle = {{DART}-{Lux}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425722000876},
	doi = {10.1016/j.rse.2022.112973},
	language = {en},
	urldate = {2024-01-15},
	journal = {Remote Sensing of Environment},
	author = {Wang, Yingjie and Kallel, Abdelaziz and Yang, Xuebo and Regaieg, Omar and Lauret, Nicolas and Guilleux, Jordan and Chavanon, Eric and Gastellu-Etchegorry, Jean-Philippe},
	month = jun,
	year = {2022},
	pages = {112973},
}

@article{abhijeet__tallavajhula_lidar_2018,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
	copyright = {In Copyright},
	shorttitle = {Lidar {Simulation} for {Robotic} {Application} {Development}},
	url = {https://kilthub.cmu.edu/articles/Lidar_Simulation_for_Robotic_Application_Development_Modeling_and_Evaluation/6720428/1},
	doi = {10.1184/R1/6720428.V1},
	abstract = {Given the increase in scale and complexity of robotics, robot application development is challenging in the real world. It may be expensive, unsafe, or impractical to collect data, or test systems, in reality. Simulation provides an answer to these challenges. In simulation, data collection is relatively inexpensive, scenes can be procedurally generated, and state information is trivially available. Despite these benefits, the use of simulators is often limited to the early stages of application development. In this work, we take steps to close the gap between simulation and reality, for Lidar simulation. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with a scene. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. We formalize the intuition that application performance must be similar in simulation and reality, using an application-level loss. Our framework is broadly applicable to simulating sensors other than Lidars. We instantiate our framework for two domains. The first domain is planar Lidar simulation in indoor scenes. We construct a high-fidelity simulator using a parametric sensor model. We show how application development paths for our simulator are closer to reality, compared to a baseline. We also pose sensor modeling as a case of distribution regression, which leads to a novel application of a nonparametric method that adapts to trends in sensor data. The second domain is Lidar simulation in o -road scenes. Our approach is to build a library of terrain primitives, derived from real Lidar observations. These are shown to generalize, resulting in an expressive simulator for complex o -road scenes. For this domain as well, we quantitatively demonstrate that our simulator is better for application development, compared to a baseline. Our work suggests a generic approach to building useful simulators. We view them as predictive models, and perform thorough tests on real data. We evaluate them with an application-level loss, which supports their greater use in the development cycle.},
	urldate = {2024-01-15},
	author = {{Abhijeet  Tallavajhula}},
	year = {2018},
	note = {Artwork Size: 14724730 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80101 Adaptive Agents and Intelligent Robotics, FOS: Computer and information sciences},
	pages = {14724730 Bytes},
}

@misc{zhang_nerf-lidar_2024,
	title = {{NeRF}-{LiDAR}: {Generating} {Realistic} {LiDAR} {Point} {Clouds} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{LiDAR}},
	url = {http://arxiv.org/abs/2304.14811},
	abstract = {Labeling LiDAR point clouds for training autonomous driving is extremely expensive and difficult. LiDAR simulation aims at generating realistic LiDAR data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages real-world information to generate realistic LIDAR point clouds. Different from existing LiDAR simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our NeRF-LiDAR by training different 3D segmentation models on the generated LiDAR point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real LiDAR data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	month = jan,
	year = {2024},
	note = {Issue: arXiv:2304.14811
arXiv:2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{guillard_learning_2022,
	title = {Learning to {Simulate} {Realistic} {LiDARs}},
	url = {http://arxiv.org/abs/2209.10986},
	abstract = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or per-point intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene's appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Guillard, Benoit and Vemprala, Sai and Gupta, Jayesh K. and Miksik, Ondrej and Vineet, Vibhav and Fua, Pascal and Kapoor, Ashish},
	month = sep,
	year = {2022},
	note = {Issue: arXiv:2209.10986
arXiv:2209.10986 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS2022 paper},
}

@article{gastellu-etchegorry_discrete_2015,
	title = {Discrete {Anisotropic} {Radiative} {Transfer} ({DART} 5) for {Modeling} {Airborne} and {Satellite} {Spectroradiometer} and {LIDAR} {Acquisitions} of {Natural} and {Urban} {Landscapes}},
	volume = {7},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/7/2/1667},
	doi = {10.3390/rs70201667},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {Remote Sensing},
	author = {Gastellu-Etchegorry, Jean-Philippe and Yin, Tiangang and Lauret, Nicolas and Cajgfinger, Thomas and Gregoire, Tristan and Grau, Eloi and Feret, Jean-Baptiste and Lopes, Maïlys and Guilleux, Jordan and Dedieu, Gérard and Malenovský, Zbyněk and Cook, Bruce and Morton, Douglas and Rubio, Jeremy and Durrieu, Sylvie and Cazanave, Gregory and Martin, Emmanuel and Ristorcelli, Thomas},
	month = feb,
	year = {2015},
	note = {Number: 2},
	pages = {1667--1701},
}

@article{jutzi_range_2006,
	title = {Range determination with waveform recording laser systems using a {Wiener} {Filter}},
	volume = {61},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271606001080},
	doi = {10.1016/j.isprsjprs.2006.09.001},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jutzi, Boris and Stilla, Uwe},
	month = nov,
	year = {2006},
	note = {Number: 2},
	pages = {95--107},
}

@article{lee_validation_2020,
	title = {{VALIDATION} {OF} {LIDAR} {CALIBRATION} {USING} {A} {LIDAR} {SIMULATOR}},
	volume = {XLIII-B1-2020},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B1-2020/39/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-39-2020},
	abstract = {Abstract. LIDAR is being widely used for mapping and modelling because it accurately scans and acquires 3D geometric information of the surrounding environment. In order to improve the accuracy of the LIDAR measurement, it is necessary to precisely estimate the intrinsic parameters as well as extrinsic parameters and eliminate the systematic errors. Many studies are conducted to eliminate these errors caused by the intrinsic parameters of LIDAR. However, when the result of intrinsic calibration is verified using actual LIDAR data, there is a problem that other error factors cannot be excluded. Therefore, in this study, the LIDAR intrinsic calibration is verified by using a LIDAR simulator that simulates the mechanism of the actual LIDAR. When constructing a LIDAR simulator, the systematic error is inserted according to the intrinsic parameter model of LIDAR. And according to the method of scanning with LIDAR, it is divided into upright scanning and tilted scanning, and the error included LIDAR simulation data is generated. After that, the intrinsic parameters are estimated by applying the plane-based intrinsic calibration. Since values of the intrinsic parameters are known, they are compared with the estimated parameters, and the results of estimate are analyzed according to the scanning method.},
	language = {en},
	urldate = {2024-01-15},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lee, G. and Cheon, J. and Lee, I.},
	month = aug,
	year = {2020},
	pages = {39--44},
}

@article{wang_measurement_2013,
	title = {Measurement and modeling of {Bidirectional} {Reflectance} {Distribution} {Function} ({BRDF}) on material surface},
	volume = {46},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224113003072},
	doi = {10.1016/j.measurement.2013.07.008},
	language = {en},
	number = {9},
	urldate = {2024-01-15},
	journal = {Measurement},
	author = {Wang, Hongyuan and Zhang, Wei and Dong, Aotuo},
	month = nov,
	year = {2013},
	note = {Number: 9},
	pages = {3654--3661},
}

@inproceedings{nakajima_lidar_2021,
	title = {{LiDAR} {Measurement} {Simulator} {Considering} {Target} {Surface} {Reflection}},
	volume = {8},
	publisher = {ESA Space Debris Office},
	author = {Nakajima, Yu and Sasaki, Takahiro and Okada, Naoki and Yamamoto, Toru},
	year = {2021},
}

@misc{heiden_physics-based_2020,
	title = {Physics-based {Simulation} of {Continuous}-{Wave} {LIDAR} for {Localization}, {Calibration} and {Tracking}},
	url = {http://arxiv.org/abs/1912.01652},
	abstract = {Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Heiden, Eric and Liu, Ziang and Ramachandran, Ragesh K. and Sukhatme, Gaurav S.},
	month = mar,
	year = {2020},
	note = {Issue: arXiv:1912.01652
arXiv:1912.01652 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Published at ICRA 2020},
}

@inproceedings{farrell_sensor_2008,
	address = {San Jose, CA},
	title = {Sensor calibration and simulation},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.767901},
	doi = {10.1117/12.767901},
	urldate = {2024-01-15},
	author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
	editor = {DiCarlo, Jeffrey M. and Rodricks, Brian G.},
	month = feb,
	year = {2008},
	pages = {68170R},
}

@incollection{gschwandtner_blensor_2011,
	address = {Berlin, Heidelberg},
	title = {{BlenSor}: {Blender} {Sensor} {Simulation} {Toolbox}},
	volume = {6939},
	isbn = {978-3-642-24030-0 978-3-642-24031-7},
	shorttitle = {{BlenSor}},
	url = {http://link.springer.com/10.1007/978-3-642-24031-7_20},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Wang, Song and Kyungnam, Kim and Benes, Bedrich and Moreland, Kenneth and Borst, Christoph and DiVerdi, Stephen and Yi-Jen, Chiang and Ming, Jiang},
	year = {2011},
	doi = {10.1007/978-3-642-24031-7_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {199--208},
}

@article{gusmao_development_2020,
	title = {Development and {Validation} of {LiDAR} {Sensor} {Simulators} {Based} on {Parallel} {Raycasting}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/24/7186},
	doi = {10.3390/s20247186},
	abstract = {Three-dimensional (3D) imaging technologies have been increasingly explored in academia and the industrial sector, especially the ones yielding point clouds. However, obtaining these data can still be expensive and time-consuming, reducing the efficiency of procedures dependent on large datasets, such as the generation of data for machine learning training, forest canopy calculation, and subsea survey. A trending solution is developing simulators for imaging systems, performing the virtual scanning of the digital world, and generating synthetic point clouds from the targets. This work presents a guideline for the development of modular Light Detection and Ranging (LiDAR) system simulators based on parallel raycasting algorithms, with its sensor modeled by metrological parameters and error models. A procedure for calibrating the sensor is also presented, based on comparing with the measurements made by a commercial LiDAR sensor. The sensor simulator developed as a case study resulted in a robust generation of synthetic point clouds in different scenarios, enabling the creation of datasets for use in concept tests, combining real and virtual data, among other applications.},
	language = {en},
	number = {24},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Gusmão, Guilherme Ferreira and Barbosa, Carlos Roberto Hall and Raposo, Alberto Barbosa},
	month = dec,
	year = {2020},
	note = {Number: 24},
	pages = {7186},
}

@article{gusmao_development_2020-1,
	title = {Development and {Validation} of {LiDAR} {Sensor} {Simulators} {Based} on {Parallel} {Raycasting}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/24/7186},
	doi = {10.3390/s20247186},
	abstract = {Three-dimensional (3D) imaging technologies have been increasingly explored in academia and the industrial sector, especially the ones yielding point clouds. However, obtaining these data can still be expensive and time-consuming, reducing the efficiency of procedures dependent on large datasets, such as the generation of data for machine learning training, forest canopy calculation, and subsea survey. A trending solution is developing simulators for imaging systems, performing the virtual scanning of the digital world, and generating synthetic point clouds from the targets. This work presents a guideline for the development of modular Light Detection and Ranging (LiDAR) system simulators based on parallel raycasting algorithms, with its sensor modeled by metrological parameters and error models. A procedure for calibrating the sensor is also presented, based on comparing with the measurements made by a commercial LiDAR sensor. The sensor simulator developed as a case study resulted in a robust generation of synthetic point clouds in different scenarios, enabling the creation of datasets for use in concept tests, combining real and virtual data, among other applications.},
	language = {en},
	number = {24},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Gusmão, Guilherme Ferreira and Barbosa, Carlos Roberto Hall and Raposo, Alberto Barbosa},
	month = dec,
	year = {2020},
	pages = {7186},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\4QK3ZTHW\\Gusmão et al. - 2020 - Development and Validation of LiDAR Sensor Simulat.pdf:application/pdf},
}

@incollection{bebis_blensor_2011,
	address = {Berlin, Heidelberg},
	title = {{BlenSor}: {Blender} {Sensor} {Simulation} {Toolbox}},
	volume = {6939},
	isbn = {978-3-642-24030-0 978-3-642-24031-7},
	shorttitle = {{BlenSor}},
	url = {http://link.springer.com/10.1007/978-3-642-24031-7_20},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Wang, Song and Kyungnam, Kim and Benes, Bedrich and Moreland, Kenneth and Borst, Christoph and DiVerdi, Stephen and Yi-Jen, Chiang and Ming, Jiang},
	year = {2011},
	doi = {10.1007/978-3-642-24031-7_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {199--208},
}

@inproceedings{farrell_sensor_2008-1,
	address = {San Jose, CA},
	title = {Sensor calibration and simulation},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.767901},
	doi = {10.1117/12.767901},
	urldate = {2024-01-15},
	author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
	editor = {DiCarlo, Jeffrey M. and Rodricks, Brian G.},
	month = feb,
	year = {2008},
	pages = {68170R},
}

@misc{heiden_physics-based_2020-1,
	title = {Physics-based {Simulation} of {Continuous}-{Wave} {LIDAR} for {Localization}, {Calibration} and {Tracking}},
	url = {http://arxiv.org/abs/1912.01652},
	abstract = {Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Heiden, Eric and Liu, Ziang and Ramachandran, Ragesh K. and Sukhatme, Gaurav S.},
	month = mar,
	year = {2020},
	note = {arXiv:1912.01652 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Published at ICRA 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\9DVKWY2W\\Heiden et al. - 2020 - Physics-based Simulation of Continuous-Wave LIDAR .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\77GC6CEX\\1912.html:text/html},
}

@inproceedings{nakajima_lidar_2021-1,
	title = {{LiDAR} {Measurement} {Simulator} {Considering} {Target} {Surface} {Reflection}},
	volume = {8},
	publisher = {ESA Space Debris Office},
	author = {Nakajima, Yu and Sasaki, Takahiro and Okada, Naoki and Yamamoto, Toru},
	year = {2021},
}

@article{wang_measurement_2013-1,
	title = {Measurement and modeling of {Bidirectional} {Reflectance} {Distribution} {Function} ({BRDF}) on material surface},
	volume = {46},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224113003072},
	doi = {10.1016/j.measurement.2013.07.008},
	language = {en},
	number = {9},
	urldate = {2024-01-15},
	journal = {Measurement},
	author = {Wang, Hongyuan and Zhang, Wei and Dong, Aotuo},
	month = nov,
	year = {2013},
	pages = {3654--3661},
}

@article{lee_validation_2020-1,
	title = {{VALIDATION} {OF} {LIDAR} {CALIBRATION} {USING} {A} {LIDAR} {SIMULATOR}},
	volume = {XLIII-B1-2020},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B1-2020/39/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-39-2020},
	abstract = {Abstract. LIDAR is being widely used for mapping and modelling because it accurately scans and acquires 3D geometric information of the surrounding environment. In order to improve the accuracy of the LIDAR measurement, it is necessary to precisely estimate the intrinsic parameters as well as extrinsic parameters and eliminate the systematic errors. Many studies are conducted to eliminate these errors caused by the intrinsic parameters of LIDAR. However, when the result of intrinsic calibration is verified using actual LIDAR data, there is a problem that other error factors cannot be excluded. Therefore, in this study, the LIDAR intrinsic calibration is verified by using a LIDAR simulator that simulates the mechanism of the actual LIDAR. When constructing a LIDAR simulator, the systematic error is inserted according to the intrinsic parameter model of LIDAR. And according to the method of scanning with LIDAR, it is divided into upright scanning and tilted scanning, and the error included LIDAR simulation data is generated. After that, the intrinsic parameters are estimated by applying the plane-based intrinsic calibration. Since values of the intrinsic parameters are known, they are compared with the estimated parameters, and the results of estimate are analyzed according to the scanning method.},
	language = {en},
	urldate = {2024-01-15},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lee, G. and Cheon, J. and Lee, I.},
	month = aug,
	year = {2020},
	pages = {39--44},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\KWMYJ35Y\\Lee et al. - 2020 - VALIDATION OF LIDAR CALIBRATION USING A LIDAR SIMU.pdf:application/pdf},
}

@article{jutzi_range_2006-1,
	title = {Range determination with waveform recording laser systems using a {Wiener} {Filter}},
	volume = {61},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271606001080},
	doi = {10.1016/j.isprsjprs.2006.09.001},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jutzi, Boris and Stilla, Uwe},
	month = nov,
	year = {2006},
	pages = {95--107},
}

@article{gastellu-etchegorry_discrete_2015-1,
	title = {Discrete {Anisotropic} {Radiative} {Transfer} ({DART} 5) for {Modeling} {Airborne} and {Satellite} {Spectroradiometer} and {LIDAR} {Acquisitions} of {Natural} and {Urban} {Landscapes}},
	volume = {7},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/7/2/1667},
	doi = {10.3390/rs70201667},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {Remote Sensing},
	author = {Gastellu-Etchegorry, Jean-Philippe and Yin, Tiangang and Lauret, Nicolas and Cajgfinger, Thomas and Gregoire, Tristan and Grau, Eloi and Feret, Jean-Baptiste and Lopes, Maïlys and Guilleux, Jordan and Dedieu, Gérard and Malenovský, Zbyněk and Cook, Bruce and Morton, Douglas and Rubio, Jeremy and Durrieu, Sylvie and Cazanave, Gregory and Martin, Emmanuel and Ristorcelli, Thomas},
	month = feb,
	year = {2015},
	pages = {1667--1701},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\ZC9XSYYI\\Gastellu-Etchegorry et al. - 2015 - Discrete Anisotropic Radiative Transfer (DART 5) f.pdf:application/pdf},
}

@misc{guillard_learning_2022-1,
	title = {Learning to {Simulate} {Realistic} {LiDARs}},
	url = {http://arxiv.org/abs/2209.10986},
	abstract = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or per-point intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene's appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Guillard, Benoit and Vemprala, Sai and Gupta, Jayesh K. and Miksik, Ondrej and Vineet, Vibhav and Fua, Pascal and Kapoor, Ashish},
	month = sep,
	year = {2022},
	note = {arXiv:2209.10986 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS2022 paper},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\ZXADUJVW\\Guillard et al. - 2022 - Learning to Simulate Realistic LiDARs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\H3REHYP7\\2209.html:text/html},
}

@misc{zhang_nerf-lidar_2024-1,
	title = {{NeRF}-{LiDAR}: {Generating} {Realistic} {LiDAR} {Point} {Clouds} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{LiDAR}},
	url = {http://arxiv.org/abs/2304.14811},
	abstract = {Labeling LiDAR point clouds for training autonomous driving is extremely expensive and difficult. LiDAR simulation aims at generating realistic LiDAR data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages real-world information to generate realistic LIDAR point clouds. Different from existing LiDAR simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our NeRF-LiDAR by training different 3D segmentation models on the generated LiDAR point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real LiDAR data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	month = jan,
	year = {2024},
	note = {arXiv:2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\QASYWDIB\\Zhang et al. - 2024 - NeRF-LiDAR Generating Realistic LiDAR Point Cloud.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\6PTJWRBF\\2304.html:text/html},
}

@misc{noauthor_gazebo_nodate,
	title = {Gazebo},
	url = {https://gazebosim.org/about},
	publisher = {Open Robotics},
}

@article{abhijeet__tallavajhula_lidar_2018-1,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
	copyright = {In Copyright},
	shorttitle = {Lidar {Simulation} for {Robotic} {Application} {Development}},
	url = {https://kilthub.cmu.edu/articles/Lidar_Simulation_for_Robotic_Application_Development_Modeling_and_Evaluation/6720428/1},
	doi = {10.1184/R1/6720428.V1},
	abstract = {Given the increase in scale and complexity of robotics, robot application development is challenging in the real world. It may be expensive, unsafe, or impractical to collect data, or test systems, in reality. Simulation provides an answer to these challenges. In simulation, data collection is relatively inexpensive, scenes can be procedurally generated, and state information is trivially available. Despite these benefits, the use of simulators is often limited to the early stages of application development. In this work, we take steps to close the gap between simulation and reality, for Lidar simulation. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with a scene. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. We formalize the intuition that application performance must be similar in simulation and reality, using an application-level loss. Our framework is broadly applicable to simulating sensors other than Lidars. We instantiate our framework for two domains. The first domain is planar Lidar simulation in indoor scenes. We construct a high-fidelity simulator using a parametric sensor model. We show how application development paths for our simulator are closer to reality, compared to a baseline. We also pose sensor modeling as a case of distribution regression, which leads to a novel application of a nonparametric method that adapts to trends in sensor data. The second domain is Lidar simulation in o -road scenes. Our approach is to build a library of terrain primitives, derived from real Lidar observations. These are shown to generalize, resulting in an expressive simulator for complex o -road scenes. For this domain as well, we quantitatively demonstrate that our simulator is better for application development, compared to a baseline. Our work suggests a generic approach to building useful simulators. We view them as predictive models, and perform thorough tests on real data. We evaluate them with an application-level loss, which supports their greater use in the development cycle.},
	urldate = {2024-01-15},
	author = {{Abhijeet  Tallavajhula}},
	year = {2018},
	note = {Artwork Size: 14724730 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80101 Adaptive Agents and Intelligent Robotics, FOS: Computer and information sciences},
	pages = {14724730 Bytes},
}

@phdthesis{noauthor_lidar_nodate,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
}

@article{wang_dart-lux_2022-1,
	title = {{DART}-{Lux}: {An} unbiased and rapid {Monte} {Carlo} radiative transfer method for simulating remote sensing images},
	volume = {274},
	issn = {00344257},
	shorttitle = {{DART}-{Lux}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425722000876},
	doi = {10.1016/j.rse.2022.112973},
	language = {en},
	urldate = {2024-01-15},
	journal = {Remote Sensing of Environment},
	author = {Wang, Yingjie and Kallel, Abdelaziz and Yang, Xuebo and Regaieg, Omar and Lauret, Nicolas and Guilleux, Jordan and Chavanon, Eric and Gastellu-Etchegorry, Jean-Philippe},
	month = jun,
	year = {2022},
	pages = {112973},
}

@article{kim_simulation_2013-1,
	title = {Simulation of a {Geiger}-{Mode} {Imaging} {LADAR} {System} for {Performance} {Assessment}},
	volume = {13},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/13/7/8461},
	doi = {10.3390/s130708461},
	language = {en},
	number = {7},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Kim, Seongjoon and Lee, Impyeong and Kwon, Yong},
	month = jul,
	year = {2013},
	pages = {8461--8489},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\95Q593RM\\Kim et al. - 2013 - Simulation of a Geiger-Mode Imaging LADAR System f.pdf:application/pdf},
}

@misc{manivasagam_lidarsim_2020-1,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	shorttitle = {{LiDARsim}},
	url = {http://arxiv.org/abs/2006.09348},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {arXiv:2006.09348 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020 (Oral)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\JLQF35SF\\Manivasagam et al. - 2020 - LiDARsim Realistic LiDAR Simulation by Leveraging.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\4DRZS8J6\\2006.html:text/html},
}

@article{lopez_gpu-accelerated_2022-1,
	title = {A {GPU}-{Accelerated} {Framework} for {Simulating} {LiDAR} {Scanning}},
	volume = {60},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9751040/},
	doi = {10.1109/TGRS.2022.3165746},
	urldate = {2024-01-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lopez, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	year = {2022},
	pages = {1--18},
}

@misc{winiwarter_virtual_2021-1,
	title = {Virtual laser scanning with {HELIOS}++: {A} novel take on ray tracing-based simulation of topographic {3D} laser scanning},
	shorttitle = {Virtual laser scanning with {HELIOS}++},
	url = {http://arxiv.org/abs/2101.09154},
	abstract = {Topographic laser scanning is a remote sensing method to create detailed 3D point cloud representations of the Earth's surface. Since data acquisition is expensive, simulations can complement real data given certain premises are available: i) a model of 3D scene and scanner, ii) a model of the beam-scene interaction, simplified to a computationally feasible while physically realistic level, and iii) an application for which simulated data is fit for use. A number of laser scanning simulators for different purposes exist, which we enrich by presenting HELIOS++. HELIOS++ is an open-source simulation framework for terrestrial static, mobile, UAV-based and airborne laser scanning implemented in C++. The HELIOS++ concept provides a flexible solution for the trade-off between physical accuracy (realism) and computational complexity (runtime, memory footprint), as well as ease of use and of configuration. Unique features of HELIOS++ include the availability of Python bindings (pyhelios) for controlling simulations, and a range of model types for 3D scene representation. HELIOS++ further allows the simulation of beam divergence using a subsampling strategy, and is able to create full-waveform outputs as a basis for detailed analysis. As generation and analysis of waveforms can strongly impact runtimes, the user may set the level of detail for the subsampling, or optionally disable full-waveform output altogether. A detailed assessment of computational considerations and a comparison of HELIOS++ to its predecessor, HELIOS, reveal reduced runtimes by up to 83 \%. At the same time, memory requirements are reduced by up to 94 \%, allowing for much larger (i.e. more complex) 3D scenes to be loaded into memory and hence to be virtually acquired by laser scanning simulation.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Winiwarter, Lukas and Pena, Alberto Manuel Esmorís and Weiser, Hannah and Anders, Katharina and Sanchez, Jorge Martínez and Searle, Mark and Höfle, Bernhard},
	month = jan,
	year = {2021},
	note = {arXiv:2101.09154 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\G7CNUTQJ\\Winiwarter et al. - 2021 - Virtual laser scanning with HELIOS++ A novel take.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\28AZLH3F\\2101.html:text/html},
}

@misc{noauthor_outsight_nodate,
	title = {Outsight {3D} {LiDAR} {Simulator}},
	url = {https://www.outsight.ai/},
	publisher = {Outsight},
}

@misc{mazzari_what_nodate,
	title = {What is {LiDAR} technology?},
	url = {https://www.generationrobots.com/blog/en/what-is-lidar-technology/},
	journal = {Generation Robots: Guides and Tutorials},
	author = {Mazzari, Vanessa},
}

@misc{noauthor_slamtec_nodate,
	title = {{SLAMTEC} {LIDAR} {ROS2} {Package}},
	url = {https://github.com/Slamtec/sllidar_ros2},
	publisher = {SLAMTEC},
}

@misc{noauthor_ros_nodate,
	title = {{ROS} 2 {Humble}},
	url = {https://docs.ros.org/en/humble/index.html},
	publisher = {Open Robotics},
}

@misc{gisgeography_what_nodate,
	title = {What is a {Point} {Cloud}?},
	url = {https://gisgeography.com/point-cloud/},
	journal = {Remote Sensing},
	author = {{GISGeography}},
}

@article{gusmao_development_2020-2,
	title = {Development and {Validation} of {LiDAR} {Sensor} {Simulators} {Based} on {Parallel} {Raycasting}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/24/7186},
	doi = {10.3390/s20247186},
	abstract = {Three-dimensional (3D) imaging technologies have been increasingly explored in academia and the industrial sector, especially the ones yielding point clouds. However, obtaining these data can still be expensive and time-consuming, reducing the efficiency of procedures dependent on large datasets, such as the generation of data for machine learning training, forest canopy calculation, and subsea survey. A trending solution is developing simulators for imaging systems, performing the virtual scanning of the digital world, and generating synthetic point clouds from the targets. This work presents a guideline for the development of modular Light Detection and Ranging (LiDAR) system simulators based on parallel raycasting algorithms, with its sensor modeled by metrological parameters and error models. A procedure for calibrating the sensor is also presented, based on comparing with the measurements made by a commercial LiDAR sensor. The sensor simulator developed as a case study resulted in a robust generation of synthetic point clouds in different scenarios, enabling the creation of datasets for use in concept tests, combining real and virtual data, among other applications.},
	language = {en},
	number = {24},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Gusmão, Guilherme Ferreira and Barbosa, Carlos Roberto Hall and Raposo, Alberto Barbosa},
	month = dec,
	year = {2020},
	note = {Number: 24},
	pages = {7186},
}

@incollection{gschwandtner_blensor_2011-1,
	address = {Berlin, Heidelberg},
	title = {{BlenSor}: {Blender} {Sensor} {Simulation} {Toolbox}},
	volume = {6939},
	isbn = {978-3-642-24030-0 978-3-642-24031-7},
	shorttitle = {{BlenSor}},
	url = {http://link.springer.com/10.1007/978-3-642-24031-7_20},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Wang, Song and Kyungnam, Kim and Benes, Bedrich and Moreland, Kenneth and Borst, Christoph and DiVerdi, Stephen and Yi-Jen, Chiang and Ming, Jiang},
	year = {2011},
	doi = {10.1007/978-3-642-24031-7_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {199--208},
}

@inproceedings{farrell_sensor_2008-2,
	address = {San Jose, CA},
	title = {Sensor calibration and simulation},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.767901},
	doi = {10.1117/12.767901},
	urldate = {2024-01-15},
	author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
	editor = {DiCarlo, Jeffrey M. and Rodricks, Brian G.},
	month = feb,
	year = {2008},
	pages = {68170R},
}

@misc{heiden_physics-based_2020-2,
	title = {Physics-based {Simulation} of {Continuous}-{Wave} {LIDAR} for {Localization}, {Calibration} and {Tracking}},
	url = {http://arxiv.org/abs/1912.01652},
	abstract = {Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Heiden, Eric and Liu, Ziang and Ramachandran, Ragesh K. and Sukhatme, Gaurav S.},
	month = mar,
	year = {2020},
	note = {Issue: arXiv:1912.01652
arXiv:1912.01652 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Published at ICRA 2020},
}

@inproceedings{nakajima_lidar_2021-2,
	title = {{LiDAR} {Measurement} {Simulator} {Considering} {Target} {Surface} {Reflection}},
	volume = {8},
	publisher = {ESA Space Debris Office},
	author = {Nakajima, Yu and Sasaki, Takahiro and Okada, Naoki and Yamamoto, Toru},
	year = {2021},
}

@article{lee_validation_2020-2,
	title = {{VALIDATION} {OF} {LIDAR} {CALIBRATION} {USING} {A} {LIDAR} {SIMULATOR}},
	volume = {XLIII-B1-2020},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B1-2020/39/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-39-2020},
	abstract = {Abstract. LIDAR is being widely used for mapping and modelling because it accurately scans and acquires 3D geometric information of the surrounding environment. In order to improve the accuracy of the LIDAR measurement, it is necessary to precisely estimate the intrinsic parameters as well as extrinsic parameters and eliminate the systematic errors. Many studies are conducted to eliminate these errors caused by the intrinsic parameters of LIDAR. However, when the result of intrinsic calibration is verified using actual LIDAR data, there is a problem that other error factors cannot be excluded. Therefore, in this study, the LIDAR intrinsic calibration is verified by using a LIDAR simulator that simulates the mechanism of the actual LIDAR. When constructing a LIDAR simulator, the systematic error is inserted according to the intrinsic parameter model of LIDAR. And according to the method of scanning with LIDAR, it is divided into upright scanning and tilted scanning, and the error included LIDAR simulation data is generated. After that, the intrinsic parameters are estimated by applying the plane-based intrinsic calibration. Since values of the intrinsic parameters are known, they are compared with the estimated parameters, and the results of estimate are analyzed according to the scanning method.},
	language = {en},
	urldate = {2024-01-15},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lee, G. and Cheon, J. and Lee, I.},
	month = aug,
	year = {2020},
	pages = {39--44},
}

@article{wang_measurement_2013-2,
	title = {Measurement and modeling of {Bidirectional} {Reflectance} {Distribution} {Function} ({BRDF}) on material surface},
	volume = {46},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224113003072},
	doi = {10.1016/j.measurement.2013.07.008},
	language = {en},
	number = {9},
	urldate = {2024-01-15},
	journal = {Measurement},
	author = {Wang, Hongyuan and Zhang, Wei and Dong, Aotuo},
	month = nov,
	year = {2013},
	note = {Number: 9},
	pages = {3654--3661},
}

@article{jutzi_range_2006-2,
	title = {Range determination with waveform recording laser systems using a {Wiener} {Filter}},
	volume = {61},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271606001080},
	doi = {10.1016/j.isprsjprs.2006.09.001},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jutzi, Boris and Stilla, Uwe},
	month = nov,
	year = {2006},
	note = {Number: 2},
	pages = {95--107},
}

@article{gastellu-etchegorry_discrete_2015-2,
	title = {Discrete {Anisotropic} {Radiative} {Transfer} ({DART} 5) for {Modeling} {Airborne} and {Satellite} {Spectroradiometer} and {LIDAR} {Acquisitions} of {Natural} and {Urban} {Landscapes}},
	volume = {7},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/7/2/1667},
	doi = {10.3390/rs70201667},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {Remote Sensing},
	author = {Gastellu-Etchegorry, Jean-Philippe and Yin, Tiangang and Lauret, Nicolas and Cajgfinger, Thomas and Gregoire, Tristan and Grau, Eloi and Feret, Jean-Baptiste and Lopes, Maïlys and Guilleux, Jordan and Dedieu, Gérard and Malenovský, Zbyněk and Cook, Bruce and Morton, Douglas and Rubio, Jeremy and Durrieu, Sylvie and Cazanave, Gregory and Martin, Emmanuel and Ristorcelli, Thomas},
	month = feb,
	year = {2015},
	note = {Number: 2},
	pages = {1667--1701},
}

@misc{zhang_nerf-lidar_2024-2,
	title = {{NeRF}-{LiDAR}: {Generating} {Realistic} {LiDAR} {Point} {Clouds} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{LiDAR}},
	url = {http://arxiv.org/abs/2304.14811},
	abstract = {Labeling LiDAR point clouds for training autonomous driving is extremely expensive and difficult. LiDAR simulation aims at generating realistic LiDAR data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages real-world information to generate realistic LIDAR point clouds. Different from existing LiDAR simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our NeRF-LiDAR by training different 3D segmentation models on the generated LiDAR point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real LiDAR data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	month = jan,
	year = {2024},
	note = {Issue: arXiv:2304.14811
arXiv:2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{abhijeet__tallavajhula_lidar_2018-2,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
	copyright = {In Copyright},
	shorttitle = {Lidar {Simulation} for {Robotic} {Application} {Development}},
	url = {https://kilthub.cmu.edu/articles/Lidar_Simulation_for_Robotic_Application_Development_Modeling_and_Evaluation/6720428/1},
	doi = {10.1184/R1/6720428.V1},
	abstract = {Given the increase in scale and complexity of robotics, robot application development is challenging in the real world. It may be expensive, unsafe, or impractical to collect data, or test systems, in reality. Simulation provides an answer to these challenges. In simulation, data collection is relatively inexpensive, scenes can be procedurally generated, and state information is trivially available. Despite these benefits, the use of simulators is often limited to the early stages of application development. In this work, we take steps to close the gap between simulation and reality, for Lidar simulation. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with a scene. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. We formalize the intuition that application performance must be similar in simulation and reality, using an application-level loss. Our framework is broadly applicable to simulating sensors other than Lidars. We instantiate our framework for two domains. The first domain is planar Lidar simulation in indoor scenes. We construct a high-fidelity simulator using a parametric sensor model. We show how application development paths for our simulator are closer to reality, compared to a baseline. We also pose sensor modeling as a case of distribution regression, which leads to a novel application of a nonparametric method that adapts to trends in sensor data. The second domain is Lidar simulation in o -road scenes. Our approach is to build a library of terrain primitives, derived from real Lidar observations. These are shown to generalize, resulting in an expressive simulator for complex o -road scenes. For this domain as well, we quantitatively demonstrate that our simulator is better for application development, compared to a baseline. Our work suggests a generic approach to building useful simulators. We view them as predictive models, and perform thorough tests on real data. We evaluate them with an application-level loss, which supports their greater use in the development cycle.},
	urldate = {2024-01-15},
	author = {{Abhijeet  Tallavajhula}},
	year = {2018},
	note = {Artwork Size: 14724730 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80101 Adaptive Agents and Intelligent Robotics, FOS: Computer and information sciences},
	pages = {14724730 Bytes},
}

@article{wang_dart-lux_2022-2,
	title = {{DART}-{Lux}: {An} unbiased and rapid {Monte} {Carlo} radiative transfer method for simulating remote sensing images},
	volume = {274},
	issn = {00344257},
	shorttitle = {{DART}-{Lux}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425722000876},
	doi = {10.1016/j.rse.2022.112973},
	language = {en},
	urldate = {2024-01-15},
	journal = {Remote Sensing of Environment},
	author = {Wang, Yingjie and Kallel, Abdelaziz and Yang, Xuebo and Regaieg, Omar and Lauret, Nicolas and Guilleux, Jordan and Chavanon, Eric and Gastellu-Etchegorry, Jean-Philippe},
	month = jun,
	year = {2022},
	pages = {112973},
}

@misc{guillard_learning_2022-2,
	title = {Learning to {Simulate} {Realistic} {LiDARs}},
	url = {http://arxiv.org/abs/2209.10986},
	abstract = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or per-point intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene's appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Guillard, Benoit and Vemprala, Sai and Gupta, Jayesh K. and Miksik, Ondrej and Vineet, Vibhav and Fua, Pascal and Kapoor, Ashish},
	month = sep,
	year = {2022},
	note = {Issue: arXiv:2209.10986
arXiv:2209.10986 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS2022 paper},
}

@article{lopez_gpu-accelerated_2022-2,
	title = {A {GPU}-{Accelerated} {Framework} for {Simulating} {LiDAR} {Scanning}},
	volume = {60},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9751040/},
	doi = {10.1109/TGRS.2022.3165746},
	urldate = {2024-01-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lopez, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	year = {2022},
	pages = {1--18},
}

@misc{winiwarter_virtual_2021-2,
	title = {Virtual laser scanning with {HELIOS}++: {A} novel take on ray tracing-based simulation of topographic {3D} laser scanning},
	shorttitle = {Virtual laser scanning with {HELIOS}++},
	url = {http://arxiv.org/abs/2101.09154},
	abstract = {Topographic laser scanning is a remote sensing method to create detailed 3D point cloud representations of the Earth's surface. Since data acquisition is expensive, simulations can complement real data given certain premises are available: i) a model of 3D scene and scanner, ii) a model of the beam-scene interaction, simplified to a computationally feasible while physically realistic level, and iii) an application for which simulated data is fit for use. A number of laser scanning simulators for different purposes exist, which we enrich by presenting HELIOS++. HELIOS++ is an open-source simulation framework for terrestrial static, mobile, UAV-based and airborne laser scanning implemented in C++. The HELIOS++ concept provides a flexible solution for the trade-off between physical accuracy (realism) and computational complexity (runtime, memory footprint), as well as ease of use and of configuration. Unique features of HELIOS++ include the availability of Python bindings (pyhelios) for controlling simulations, and a range of model types for 3D scene representation. HELIOS++ further allows the simulation of beam divergence using a subsampling strategy, and is able to create full-waveform outputs as a basis for detailed analysis. As generation and analysis of waveforms can strongly impact runtimes, the user may set the level of detail for the subsampling, or optionally disable full-waveform output altogether. A detailed assessment of computational considerations and a comparison of HELIOS++ to its predecessor, HELIOS, reveal reduced runtimes by up to 83 \%. At the same time, memory requirements are reduced by up to 94 \%, allowing for much larger (i.e. more complex) 3D scenes to be loaded into memory and hence to be virtually acquired by laser scanning simulation.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Winiwarter, Lukas and Pena, Alberto Manuel Esmorís and Weiser, Hannah and Anders, Katharina and Sanchez, Jorge Martínez and Searle, Mark and Höfle, Bernhard},
	month = jan,
	year = {2021},
	note = {Issue: arXiv:2101.09154
arXiv:2101.09154 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{kim_simulation_2013-2,
	title = {Simulation of a {Geiger}-{Mode} {Imaging} {LADAR} {System} for {Performance} {Assessment}},
	volume = {13},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/13/7/8461},
	doi = {10.3390/s130708461},
	language = {en},
	number = {7},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Kim, Seongjoon and Lee, Impyeong and Kwon, Yong},
	month = jul,
	year = {2013},
	note = {Number: 7},
	pages = {8461--8489},
}

@misc{manivasagam_lidarsim_2020-2,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	shorttitle = {{LiDARsim}},
	url = {http://arxiv.org/abs/2006.09348},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {Issue: arXiv:2006.09348
arXiv:2006.09348 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020 (Oral)},
}

@misc{noauthor_velodyne_2019,
	title = {Velodyne {Lidar} {ULTRA} {Puck}},
	url = {https://velodynelidar.com/wp-content/uploads/2019/12/63-9378_Rev-F_Ultra-Puck_Datasheet_Web.pdf},
	publisher = {Velodyne Lidar, Inc},
	year = {2019},
}

@book{patricio_gonzalez_vivo_book_nodate,
	title = {The {Book} of {Shaders}},
	url = {https://thebookofshaders.com/},
	author = {{Patricio Gonzalez Vivo} and {Jen Lowe}},
}

@book{joey_de_vries_learn_nodate,
	title = {Learn {OpenGL} - {Graphics} {Programming}},
	url = {https://learnopengl.com},
	author = {{Joey de Vries}},
}

@book{vries_learn_2020,
	address = {Erscheinungsort nicht ermittelbar},
	title = {Learn {OpenGL} - {Graphics} programming: {Learn} modern {OpenGL} graphics programming in a step-by-step fashion},
	isbn = {978-90-90-33256-7},
	shorttitle = {Learn {OpenGL} - {Graphics} programming},
	abstract = {Graphics programmers are often coined the 'wizards' of the game industry. As every magician knows, terms like wizardry and magic are often somewhat exaggerated. Yet, there is a certain charm to graphics programming: the ability to conjure up complete living worlds at our fingertips. Learn OpenGL will teach you the basics, the intermediate, and tons of advanced knowledge, using modern (core-profile) OpenGL. The aim of this book is to show you all there is to modern OpenGL in an easy-to-understand fashion, with clear examples and step-by-step instructions, while also providing a useful reference for later studies. After years of continuous work and improvements on the accompanying website learnopengl.com, with the help of thousands of readers, its content has been professionally revised for this physical copy you now find in your hands. Graphics programming isn't as hard as many people make it out to be... you just need to start},
	language = {eng},
	publisher = {Kendall \& Welling},
	author = {Vries, Joey de},
	year = {2020},
	annote = {Includes index},
}

@misc{noauthor_carla_nodate,
	title = {{CARLA} {Simulator}},
	url = {https://carla.org//},
}
