
@article{noauthor_notitle_nodate,
}

@misc{winiwarter_virtual_2021,
	title = {Virtual laser scanning with {HELIOS}++: {A} novel take on ray tracing-based simulation of topographic {3D} laser scanning},
	shorttitle = {Virtual laser scanning with {HELIOS}++},
	url = {http://arxiv.org/abs/2101.09154},
	abstract = {Topographic laser scanning is a remote sensing method to create detailed 3D point cloud representations of the Earth's surface. Since data acquisition is expensive, simulations can complement real data given certain premises are available: i) a model of 3D scene and scanner, ii) a model of the beam-scene interaction, simplified to a computationally feasible while physically realistic level, and iii) an application for which simulated data is fit for use. A number of laser scanning simulators for different purposes exist, which we enrich by presenting HELIOS++. HELIOS++ is an open-source simulation framework for terrestrial static, mobile, UAV-based and airborne laser scanning implemented in C++. The HELIOS++ concept provides a flexible solution for the trade-off between physical accuracy (realism) and computational complexity (runtime, memory footprint), as well as ease of use and of configuration. Unique features of HELIOS++ include the availability of Python bindings (pyhelios) for controlling simulations, and a range of model types for 3D scene representation. HELIOS++ further allows the simulation of beam divergence using a subsampling strategy, and is able to create full-waveform outputs as a basis for detailed analysis. As generation and analysis of waveforms can strongly impact runtimes, the user may set the level of detail for the subsampling, or optionally disable full-waveform output altogether. A detailed assessment of computational considerations and a comparison of HELIOS++ to its predecessor, HELIOS, reveal reduced runtimes by up to 83 \%. At the same time, memory requirements are reduced by up to 94 \%, allowing for much larger (i.e. more complex) 3D scenes to be loaded into memory and hence to be virtually acquired by laser scanning simulation.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Winiwarter, Lukas and Pena, Alberto Manuel Esmorís and Weiser, Hannah and Anders, Katharina and Sanchez, Jorge Martínez and Searle, Mark and Höfle, Bernhard},
	month = jan,
	year = {2021},
	note = {Issue: arXiv:2101.09154
arXiv:2101.09154 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{lopez_gpu-accelerated_2022,
	title = {A {GPU}-{Accelerated} {Framework} for {Simulating} {LiDAR} {Scanning}},
	volume = {60},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9751040/},
	doi = {10.1109/TGRS.2022.3165746},
	urldate = {2024-01-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lopez, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	year = {2022},
	pages = {1--18},
}

@misc{manivasagam_lidarsim_2020,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	shorttitle = {{LiDARsim}},
	url = {http://arxiv.org/abs/2006.09348},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {Issue: arXiv:2006.09348
arXiv:2006.09348 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020 (Oral)},
}

@article{kim_simulation_2013,
	title = {Simulation of a {Geiger}-{Mode} {Imaging} {LADAR} {System} for {Performance} {Assessment}},
	volume = {13},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/13/7/8461},
	doi = {10.3390/s130708461},
	language = {en},
	number = {7},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Kim, Seongjoon and Lee, Impyeong and Kwon, Yong},
	month = jul,
	year = {2013},
	note = {Number: 7},
	pages = {8461--8489},
}

@article{wang_dart-lux_2022,
	title = {{DART}-{Lux}: {An} unbiased and rapid {Monte} {Carlo} radiative transfer method for simulating remote sensing images},
	volume = {274},
	issn = {00344257},
	shorttitle = {{DART}-{Lux}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425722000876},
	doi = {10.1016/j.rse.2022.112973},
	language = {en},
	urldate = {2024-01-15},
	journal = {Remote Sensing of Environment},
	author = {Wang, Yingjie and Kallel, Abdelaziz and Yang, Xuebo and Regaieg, Omar and Lauret, Nicolas and Guilleux, Jordan and Chavanon, Eric and Gastellu-Etchegorry, Jean-Philippe},
	month = jun,
	year = {2022},
	pages = {112973},
}

@article{abhijeet__tallavajhula_lidar_2018,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
	copyright = {In Copyright},
	shorttitle = {Lidar {Simulation} for {Robotic} {Application} {Development}},
	url = {https://kilthub.cmu.edu/articles/Lidar_Simulation_for_Robotic_Application_Development_Modeling_and_Evaluation/6720428/1},
	doi = {10.1184/R1/6720428.V1},
	abstract = {Given the increase in scale and complexity of robotics, robot application development is challenging in the real world. It may be expensive, unsafe, or impractical to collect data, or test systems, in reality. Simulation provides an answer to these challenges. In simulation, data collection is relatively inexpensive, scenes can be procedurally generated, and state information is trivially available. Despite these benefits, the use of simulators is often limited to the early stages of application development. In this work, we take steps to close the gap between simulation and reality, for Lidar simulation. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with a scene. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. We formalize the intuition that application performance must be similar in simulation and reality, using an application-level loss. Our framework is broadly applicable to simulating sensors other than Lidars. We instantiate our framework for two domains. The first domain is planar Lidar simulation in indoor scenes. We construct a high-fidelity simulator using a parametric sensor model. We show how application development paths for our simulator are closer to reality, compared to a baseline. We also pose sensor modeling as a case of distribution regression, which leads to a novel application of a nonparametric method that adapts to trends in sensor data. The second domain is Lidar simulation in o -road scenes. Our approach is to build a library of terrain primitives, derived from real Lidar observations. These are shown to generalize, resulting in an expressive simulator for complex o -road scenes. For this domain as well, we quantitatively demonstrate that our simulator is better for application development, compared to a baseline. Our work suggests a generic approach to building useful simulators. We view them as predictive models, and perform thorough tests on real data. We evaluate them with an application-level loss, which supports their greater use in the development cycle.},
	urldate = {2024-01-15},
	author = {{Abhijeet  Tallavajhula}},
	year = {2018},
	note = {Artwork Size: 14724730 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80101 Adaptive Agents and Intelligent Robotics, FOS: Computer and information sciences},
	pages = {14724730 Bytes},
}

@misc{zhang_nerf-lidar_2024,
	title = {{NeRF}-{LiDAR}: {Generating} {Realistic} {LiDAR} {Point} {Clouds} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{LiDAR}},
	url = {http://arxiv.org/abs/2304.14811},
	abstract = {Labeling LiDAR point clouds for training autonomous driving is extremely expensive and difficult. LiDAR simulation aims at generating realistic LiDAR data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages real-world information to generate realistic LIDAR point clouds. Different from existing LiDAR simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our NeRF-LiDAR by training different 3D segmentation models on the generated LiDAR point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real LiDAR data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	month = jan,
	year = {2024},
	note = {Issue: arXiv:2304.14811
arXiv:2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{guillard_learning_2022,
	title = {Learning to {Simulate} {Realistic} {LiDARs}},
	url = {http://arxiv.org/abs/2209.10986},
	abstract = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or per-point intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene's appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Guillard, Benoit and Vemprala, Sai and Gupta, Jayesh K. and Miksik, Ondrej and Vineet, Vibhav and Fua, Pascal and Kapoor, Ashish},
	month = sep,
	year = {2022},
	note = {Issue: arXiv:2209.10986
arXiv:2209.10986 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS2022 paper},
}

@article{gastellu-etchegorry_discrete_2015,
	title = {Discrete {Anisotropic} {Radiative} {Transfer} ({DART} 5) for {Modeling} {Airborne} and {Satellite} {Spectroradiometer} and {LIDAR} {Acquisitions} of {Natural} and {Urban} {Landscapes}},
	volume = {7},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/7/2/1667},
	doi = {10.3390/rs70201667},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {Remote Sensing},
	author = {Gastellu-Etchegorry, Jean-Philippe and Yin, Tiangang and Lauret, Nicolas and Cajgfinger, Thomas and Gregoire, Tristan and Grau, Eloi and Feret, Jean-Baptiste and Lopes, Maïlys and Guilleux, Jordan and Dedieu, Gérard and Malenovský, Zbyněk and Cook, Bruce and Morton, Douglas and Rubio, Jeremy and Durrieu, Sylvie and Cazanave, Gregory and Martin, Emmanuel and Ristorcelli, Thomas},
	month = feb,
	year = {2015},
	note = {Number: 2},
	pages = {1667--1701},
}

@article{jutzi_range_2006,
	title = {Range determination with waveform recording laser systems using a {Wiener} {Filter}},
	volume = {61},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271606001080},
	doi = {10.1016/j.isprsjprs.2006.09.001},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jutzi, Boris and Stilla, Uwe},
	month = nov,
	year = {2006},
	note = {Number: 2},
	pages = {95--107},
}

@article{lee_validation_2020,
	title = {{VALIDATION} {OF} {LIDAR} {CALIBRATION} {USING} {A} {LIDAR} {SIMULATOR}},
	volume = {XLIII-B1-2020},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B1-2020/39/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-39-2020},
	abstract = {Abstract. LIDAR is being widely used for mapping and modelling because it accurately scans and acquires 3D geometric information of the surrounding environment. In order to improve the accuracy of the LIDAR measurement, it is necessary to precisely estimate the intrinsic parameters as well as extrinsic parameters and eliminate the systematic errors. Many studies are conducted to eliminate these errors caused by the intrinsic parameters of LIDAR. However, when the result of intrinsic calibration is verified using actual LIDAR data, there is a problem that other error factors cannot be excluded. Therefore, in this study, the LIDAR intrinsic calibration is verified by using a LIDAR simulator that simulates the mechanism of the actual LIDAR. When constructing a LIDAR simulator, the systematic error is inserted according to the intrinsic parameter model of LIDAR. And according to the method of scanning with LIDAR, it is divided into upright scanning and tilted scanning, and the error included LIDAR simulation data is generated. After that, the intrinsic parameters are estimated by applying the plane-based intrinsic calibration. Since values of the intrinsic parameters are known, they are compared with the estimated parameters, and the results of estimate are analyzed according to the scanning method.},
	language = {en},
	urldate = {2024-01-15},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lee, G. and Cheon, J. and Lee, I.},
	month = aug,
	year = {2020},
	pages = {39--44},
}

@article{wang_measurement_2013,
	title = {Measurement and modeling of {Bidirectional} {Reflectance} {Distribution} {Function} ({BRDF}) on material surface},
	volume = {46},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224113003072},
	doi = {10.1016/j.measurement.2013.07.008},
	language = {en},
	number = {9},
	urldate = {2024-01-15},
	journal = {Measurement},
	author = {Wang, Hongyuan and Zhang, Wei and Dong, Aotuo},
	month = nov,
	year = {2013},
	note = {Number: 9},
	pages = {3654--3661},
}

@inproceedings{nakajima_lidar_2021,
	title = {{LiDAR} {Measurement} {Simulator} {Considering} {Target} {Surface} {Reflection}},
	volume = {8},
	publisher = {ESA Space Debris Office},
	author = {Nakajima, Yu and Sasaki, Takahiro and Okada, Naoki and Yamamoto, Toru},
	year = {2021},
}

@misc{heiden_physics-based_2020,
	title = {Physics-based {Simulation} of {Continuous}-{Wave} {LIDAR} for {Localization}, {Calibration} and {Tracking}},
	url = {http://arxiv.org/abs/1912.01652},
	abstract = {Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Heiden, Eric and Liu, Ziang and Ramachandran, Ragesh K. and Sukhatme, Gaurav S.},
	month = mar,
	year = {2020},
	note = {Issue: arXiv:1912.01652
arXiv:1912.01652 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Published at ICRA 2020},
}

@inproceedings{farrell_sensor_2008,
	address = {San Jose, CA},
	title = {Sensor calibration and simulation},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.767901},
	doi = {10.1117/12.767901},
	urldate = {2024-01-15},
	author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
	editor = {DiCarlo, Jeffrey M. and Rodricks, Brian G.},
	month = feb,
	year = {2008},
	pages = {68170R},
}

@incollection{gschwandtner_blensor_2011,
	address = {Berlin, Heidelberg},
	title = {{BlenSor}: {Blender} {Sensor} {Simulation} {Toolbox}},
	volume = {6939},
	isbn = {978-3-642-24030-0 978-3-642-24031-7},
	shorttitle = {{BlenSor}},
	url = {http://link.springer.com/10.1007/978-3-642-24031-7_20},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Wang, Song and Kyungnam, Kim and Benes, Bedrich and Moreland, Kenneth and Borst, Christoph and DiVerdi, Stephen and Yi-Jen, Chiang and Ming, Jiang},
	year = {2011},
	doi = {10.1007/978-3-642-24031-7_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {199--208},
}

@article{gusmao_development_2020,
	title = {Development and {Validation} of {LiDAR} {Sensor} {Simulators} {Based} on {Parallel} {Raycasting}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/24/7186},
	doi = {10.3390/s20247186},
	abstract = {Three-dimensional (3D) imaging technologies have been increasingly explored in academia and the industrial sector, especially the ones yielding point clouds. However, obtaining these data can still be expensive and time-consuming, reducing the efficiency of procedures dependent on large datasets, such as the generation of data for machine learning training, forest canopy calculation, and subsea survey. A trending solution is developing simulators for imaging systems, performing the virtual scanning of the digital world, and generating synthetic point clouds from the targets. This work presents a guideline for the development of modular Light Detection and Ranging (LiDAR) system simulators based on parallel raycasting algorithms, with its sensor modeled by metrological parameters and error models. A procedure for calibrating the sensor is also presented, based on comparing with the measurements made by a commercial LiDAR sensor. The sensor simulator developed as a case study resulted in a robust generation of synthetic point clouds in different scenarios, enabling the creation of datasets for use in concept tests, combining real and virtual data, among other applications.},
	language = {en},
	number = {24},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Gusmão, Guilherme Ferreira and Barbosa, Carlos Roberto Hall and Raposo, Alberto Barbosa},
	month = dec,
	year = {2020},
	note = {Number: 24},
	pages = {7186},
}

@article{gusmao_development_2020-1,
	title = {Development and {Validation} of {LiDAR} {Sensor} {Simulators} {Based} on {Parallel} {Raycasting}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/24/7186},
	doi = {10.3390/s20247186},
	abstract = {Three-dimensional (3D) imaging technologies have been increasingly explored in academia and the industrial sector, especially the ones yielding point clouds. However, obtaining these data can still be expensive and time-consuming, reducing the efficiency of procedures dependent on large datasets, such as the generation of data for machine learning training, forest canopy calculation, and subsea survey. A trending solution is developing simulators for imaging systems, performing the virtual scanning of the digital world, and generating synthetic point clouds from the targets. This work presents a guideline for the development of modular Light Detection and Ranging (LiDAR) system simulators based on parallel raycasting algorithms, with its sensor modeled by metrological parameters and error models. A procedure for calibrating the sensor is also presented, based on comparing with the measurements made by a commercial LiDAR sensor. The sensor simulator developed as a case study resulted in a robust generation of synthetic point clouds in different scenarios, enabling the creation of datasets for use in concept tests, combining real and virtual data, among other applications.},
	language = {en},
	number = {24},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Gusmão, Guilherme Ferreira and Barbosa, Carlos Roberto Hall and Raposo, Alberto Barbosa},
	month = dec,
	year = {2020},
	pages = {7186},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\4QK3ZTHW\\Gusmão et al. - 2020 - Development and Validation of LiDAR Sensor Simulat.pdf:application/pdf},
}

@incollection{bebis_blensor_2011,
	address = {Berlin, Heidelberg},
	title = {{BlenSor}: {Blender} {Sensor} {Simulation} {Toolbox}},
	volume = {6939},
	isbn = {978-3-642-24030-0 978-3-642-24031-7},
	shorttitle = {{BlenSor}},
	url = {http://link.springer.com/10.1007/978-3-642-24031-7_20},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Wang, Song and Kyungnam, Kim and Benes, Bedrich and Moreland, Kenneth and Borst, Christoph and DiVerdi, Stephen and Yi-Jen, Chiang and Ming, Jiang},
	year = {2011},
	doi = {10.1007/978-3-642-24031-7_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {199--208},
}

@inproceedings{farrell_sensor_2008-1,
	address = {San Jose, CA},
	title = {Sensor calibration and simulation},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.767901},
	doi = {10.1117/12.767901},
	urldate = {2024-01-15},
	author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
	editor = {DiCarlo, Jeffrey M. and Rodricks, Brian G.},
	month = feb,
	year = {2008},
	pages = {68170R},
}

@misc{heiden_physics-based_2020-1,
	title = {Physics-based {Simulation} of {Continuous}-{Wave} {LIDAR} for {Localization}, {Calibration} and {Tracking}},
	url = {http://arxiv.org/abs/1912.01652},
	abstract = {Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Heiden, Eric and Liu, Ziang and Ramachandran, Ragesh K. and Sukhatme, Gaurav S.},
	month = mar,
	year = {2020},
	note = {arXiv:1912.01652 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Published at ICRA 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\9DVKWY2W\\Heiden et al. - 2020 - Physics-based Simulation of Continuous-Wave LIDAR .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\77GC6CEX\\1912.html:text/html},
}

@inproceedings{nakajima_lidar_2021-1,
	title = {{LiDAR} {Measurement} {Simulator} {Considering} {Target} {Surface} {Reflection}},
	volume = {8},
	publisher = {ESA Space Debris Office},
	author = {Nakajima, Yu and Sasaki, Takahiro and Okada, Naoki and Yamamoto, Toru},
	year = {2021},
}

@article{wang_measurement_2013-1,
	title = {Measurement and modeling of {Bidirectional} {Reflectance} {Distribution} {Function} ({BRDF}) on material surface},
	volume = {46},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224113003072},
	doi = {10.1016/j.measurement.2013.07.008},
	language = {en},
	number = {9},
	urldate = {2024-01-15},
	journal = {Measurement},
	author = {Wang, Hongyuan and Zhang, Wei and Dong, Aotuo},
	month = nov,
	year = {2013},
	pages = {3654--3661},
}

@article{lee_validation_2020-1,
	title = {{VALIDATION} {OF} {LIDAR} {CALIBRATION} {USING} {A} {LIDAR} {SIMULATOR}},
	volume = {XLIII-B1-2020},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B1-2020/39/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-39-2020},
	abstract = {Abstract. LIDAR is being widely used for mapping and modelling because it accurately scans and acquires 3D geometric information of the surrounding environment. In order to improve the accuracy of the LIDAR measurement, it is necessary to precisely estimate the intrinsic parameters as well as extrinsic parameters and eliminate the systematic errors. Many studies are conducted to eliminate these errors caused by the intrinsic parameters of LIDAR. However, when the result of intrinsic calibration is verified using actual LIDAR data, there is a problem that other error factors cannot be excluded. Therefore, in this study, the LIDAR intrinsic calibration is verified by using a LIDAR simulator that simulates the mechanism of the actual LIDAR. When constructing a LIDAR simulator, the systematic error is inserted according to the intrinsic parameter model of LIDAR. And according to the method of scanning with LIDAR, it is divided into upright scanning and tilted scanning, and the error included LIDAR simulation data is generated. After that, the intrinsic parameters are estimated by applying the plane-based intrinsic calibration. Since values of the intrinsic parameters are known, they are compared with the estimated parameters, and the results of estimate are analyzed according to the scanning method.},
	language = {en},
	urldate = {2024-01-15},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lee, G. and Cheon, J. and Lee, I.},
	month = aug,
	year = {2020},
	pages = {39--44},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\KWMYJ35Y\\Lee et al. - 2020 - VALIDATION OF LIDAR CALIBRATION USING A LIDAR SIMU.pdf:application/pdf},
}

@article{jutzi_range_2006-1,
	title = {Range determination with waveform recording laser systems using a {Wiener} {Filter}},
	volume = {61},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271606001080},
	doi = {10.1016/j.isprsjprs.2006.09.001},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jutzi, Boris and Stilla, Uwe},
	month = nov,
	year = {2006},
	pages = {95--107},
}

@article{gastellu-etchegorry_discrete_2015-1,
	title = {Discrete {Anisotropic} {Radiative} {Transfer} ({DART} 5) for {Modeling} {Airborne} and {Satellite} {Spectroradiometer} and {LIDAR} {Acquisitions} of {Natural} and {Urban} {Landscapes}},
	volume = {7},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/7/2/1667},
	doi = {10.3390/rs70201667},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {Remote Sensing},
	author = {Gastellu-Etchegorry, Jean-Philippe and Yin, Tiangang and Lauret, Nicolas and Cajgfinger, Thomas and Gregoire, Tristan and Grau, Eloi and Feret, Jean-Baptiste and Lopes, Maïlys and Guilleux, Jordan and Dedieu, Gérard and Malenovský, Zbyněk and Cook, Bruce and Morton, Douglas and Rubio, Jeremy and Durrieu, Sylvie and Cazanave, Gregory and Martin, Emmanuel and Ristorcelli, Thomas},
	month = feb,
	year = {2015},
	pages = {1667--1701},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\ZC9XSYYI\\Gastellu-Etchegorry et al. - 2015 - Discrete Anisotropic Radiative Transfer (DART 5) f.pdf:application/pdf},
}

@misc{guillard_learning_2022-1,
	title = {Learning to {Simulate} {Realistic} {LiDARs}},
	url = {http://arxiv.org/abs/2209.10986},
	abstract = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or per-point intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene's appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Guillard, Benoit and Vemprala, Sai and Gupta, Jayesh K. and Miksik, Ondrej and Vineet, Vibhav and Fua, Pascal and Kapoor, Ashish},
	month = sep,
	year = {2022},
	note = {arXiv:2209.10986 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS2022 paper},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\ZXADUJVW\\Guillard et al. - 2022 - Learning to Simulate Realistic LiDARs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\H3REHYP7\\2209.html:text/html},
}

@misc{zhang_nerf-lidar_2024-1,
	title = {{NeRF}-{LiDAR}: {Generating} {Realistic} {LiDAR} {Point} {Clouds} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{LiDAR}},
	url = {http://arxiv.org/abs/2304.14811},
	abstract = {Labeling LiDAR point clouds for training autonomous driving is extremely expensive and difficult. LiDAR simulation aims at generating realistic LiDAR data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages real-world information to generate realistic LIDAR point clouds. Different from existing LiDAR simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our NeRF-LiDAR by training different 3D segmentation models on the generated LiDAR point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real LiDAR data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	month = jan,
	year = {2024},
	note = {arXiv:2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\QASYWDIB\\Zhang et al. - 2024 - NeRF-LiDAR Generating Realistic LiDAR Point Cloud.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\6PTJWRBF\\2304.html:text/html},
}

@misc{noauthor_gazebo_nodate,
	title = {Gazebo},
	url = {https://gazebosim.org/about},
	publisher = {Open Robotics},
}

@article{abhijeet__tallavajhula_lidar_2018-1,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
	copyright = {In Copyright},
	shorttitle = {Lidar {Simulation} for {Robotic} {Application} {Development}},
	url = {https://kilthub.cmu.edu/articles/Lidar_Simulation_for_Robotic_Application_Development_Modeling_and_Evaluation/6720428/1},
	doi = {10.1184/R1/6720428.V1},
	abstract = {Given the increase in scale and complexity of robotics, robot application development is challenging in the real world. It may be expensive, unsafe, or impractical to collect data, or test systems, in reality. Simulation provides an answer to these challenges. In simulation, data collection is relatively inexpensive, scenes can be procedurally generated, and state information is trivially available. Despite these benefits, the use of simulators is often limited to the early stages of application development. In this work, we take steps to close the gap between simulation and reality, for Lidar simulation. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with a scene. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. We formalize the intuition that application performance must be similar in simulation and reality, using an application-level loss. Our framework is broadly applicable to simulating sensors other than Lidars. We instantiate our framework for two domains. The first domain is planar Lidar simulation in indoor scenes. We construct a high-fidelity simulator using a parametric sensor model. We show how application development paths for our simulator are closer to reality, compared to a baseline. We also pose sensor modeling as a case of distribution regression, which leads to a novel application of a nonparametric method that adapts to trends in sensor data. The second domain is Lidar simulation in o -road scenes. Our approach is to build a library of terrain primitives, derived from real Lidar observations. These are shown to generalize, resulting in an expressive simulator for complex o -road scenes. For this domain as well, we quantitatively demonstrate that our simulator is better for application development, compared to a baseline. Our work suggests a generic approach to building useful simulators. We view them as predictive models, and perform thorough tests on real data. We evaluate them with an application-level loss, which supports their greater use in the development cycle.},
	urldate = {2024-01-15},
	author = {{Abhijeet  Tallavajhula}},
	year = {2018},
	note = {Artwork Size: 14724730 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80101 Adaptive Agents and Intelligent Robotics, FOS: Computer and information sciences},
	pages = {14724730 Bytes},
}

@phdthesis{noauthor_lidar_nodate,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
}

@article{wang_dart-lux_2022-1,
	title = {{DART}-{Lux}: {An} unbiased and rapid {Monte} {Carlo} radiative transfer method for simulating remote sensing images},
	volume = {274},
	issn = {00344257},
	shorttitle = {{DART}-{Lux}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425722000876},
	doi = {10.1016/j.rse.2022.112973},
	language = {en},
	urldate = {2024-01-15},
	journal = {Remote Sensing of Environment},
	author = {Wang, Yingjie and Kallel, Abdelaziz and Yang, Xuebo and Regaieg, Omar and Lauret, Nicolas and Guilleux, Jordan and Chavanon, Eric and Gastellu-Etchegorry, Jean-Philippe},
	month = jun,
	year = {2022},
	pages = {112973},
}

@article{kim_simulation_2013-1,
	title = {Simulation of a {Geiger}-{Mode} {Imaging} {LADAR} {System} for {Performance} {Assessment}},
	volume = {13},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/13/7/8461},
	doi = {10.3390/s130708461},
	language = {en},
	number = {7},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Kim, Seongjoon and Lee, Impyeong and Kwon, Yong},
	month = jul,
	year = {2013},
	pages = {8461--8489},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\95Q593RM\\Kim et al. - 2013 - Simulation of a Geiger-Mode Imaging LADAR System f.pdf:application/pdf},
}

@misc{manivasagam_lidarsim_2020-1,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	shorttitle = {{LiDARsim}},
	url = {http://arxiv.org/abs/2006.09348},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {arXiv:2006.09348 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020 (Oral)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\JLQF35SF\\Manivasagam et al. - 2020 - LiDARsim Realistic LiDAR Simulation by Leveraging.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\4DRZS8J6\\2006.html:text/html},
}

@article{lopez_gpu-accelerated_2022-1,
	title = {A {GPU}-{Accelerated} {Framework} for {Simulating} {LiDAR} {Scanning}},
	volume = {60},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9751040/},
	doi = {10.1109/TGRS.2022.3165746},
	urldate = {2024-01-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lopez, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	year = {2022},
	pages = {1--18},
}

@misc{winiwarter_virtual_2021-1,
	title = {Virtual laser scanning with {HELIOS}++: {A} novel take on ray tracing-based simulation of topographic {3D} laser scanning},
	shorttitle = {Virtual laser scanning with {HELIOS}++},
	url = {http://arxiv.org/abs/2101.09154},
	abstract = {Topographic laser scanning is a remote sensing method to create detailed 3D point cloud representations of the Earth's surface. Since data acquisition is expensive, simulations can complement real data given certain premises are available: i) a model of 3D scene and scanner, ii) a model of the beam-scene interaction, simplified to a computationally feasible while physically realistic level, and iii) an application for which simulated data is fit for use. A number of laser scanning simulators for different purposes exist, which we enrich by presenting HELIOS++. HELIOS++ is an open-source simulation framework for terrestrial static, mobile, UAV-based and airborne laser scanning implemented in C++. The HELIOS++ concept provides a flexible solution for the trade-off between physical accuracy (realism) and computational complexity (runtime, memory footprint), as well as ease of use and of configuration. Unique features of HELIOS++ include the availability of Python bindings (pyhelios) for controlling simulations, and a range of model types for 3D scene representation. HELIOS++ further allows the simulation of beam divergence using a subsampling strategy, and is able to create full-waveform outputs as a basis for detailed analysis. As generation and analysis of waveforms can strongly impact runtimes, the user may set the level of detail for the subsampling, or optionally disable full-waveform output altogether. A detailed assessment of computational considerations and a comparison of HELIOS++ to its predecessor, HELIOS, reveal reduced runtimes by up to 83 \%. At the same time, memory requirements are reduced by up to 94 \%, allowing for much larger (i.e. more complex) 3D scenes to be loaded into memory and hence to be virtually acquired by laser scanning simulation.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Winiwarter, Lukas and Pena, Alberto Manuel Esmorís and Weiser, Hannah and Anders, Katharina and Sanchez, Jorge Martínez and Searle, Mark and Höfle, Bernhard},
	month = jan,
	year = {2021},
	note = {arXiv:2101.09154 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\G7CNUTQJ\\Winiwarter et al. - 2021 - Virtual laser scanning with HELIOS++ A novel take.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\28AZLH3F\\2101.html:text/html},
}

@misc{noauthor_outsight_nodate,
	title = {Outsight {3D} {LiDAR} {Simulator}},
	url = {https://www.outsight.ai/},
	publisher = {Outsight},
}

@misc{mazzari_what_nodate,
	title = {What is {LiDAR} technology?},
	url = {https://www.generationrobots.com/blog/en/what-is-lidar-technology/},
	journal = {Generation Robots: Guides and Tutorials},
	author = {Mazzari, Vanessa},
}

@misc{noauthor_slamtec_nodate,
	title = {{SLAMTEC} {LIDAR} {ROS2} {Package}},
	url = {https://github.com/Slamtec/sllidar_ros2},
	publisher = {SLAMTEC},
}

@misc{noauthor_ros_nodate,
	title = {{ROS} 2 {Humble}},
	url = {https://docs.ros.org/en/humble/index.html},
	publisher = {Open Robotics},
}

@misc{gisgeography_what_nodate,
	title = {What is a {Point} {Cloud}?},
	url = {https://gisgeography.com/point-cloud/},
	journal = {Remote Sensing},
	author = {{GISGeography}},
}

@article{gusmao_development_2020-2,
	title = {Development and {Validation} of {LiDAR} {Sensor} {Simulators} {Based} on {Parallel} {Raycasting}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/24/7186},
	doi = {10.3390/s20247186},
	abstract = {Three-dimensional (3D) imaging technologies have been increasingly explored in academia and the industrial sector, especially the ones yielding point clouds. However, obtaining these data can still be expensive and time-consuming, reducing the efficiency of procedures dependent on large datasets, such as the generation of data for machine learning training, forest canopy calculation, and subsea survey. A trending solution is developing simulators for imaging systems, performing the virtual scanning of the digital world, and generating synthetic point clouds from the targets. This work presents a guideline for the development of modular Light Detection and Ranging (LiDAR) system simulators based on parallel raycasting algorithms, with its sensor modeled by metrological parameters and error models. A procedure for calibrating the sensor is also presented, based on comparing with the measurements made by a commercial LiDAR sensor. The sensor simulator developed as a case study resulted in a robust generation of synthetic point clouds in different scenarios, enabling the creation of datasets for use in concept tests, combining real and virtual data, among other applications.},
	language = {en},
	number = {24},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Gusmão, Guilherme Ferreira and Barbosa, Carlos Roberto Hall and Raposo, Alberto Barbosa},
	month = dec,
	year = {2020},
	note = {Number: 24},
	pages = {7186},
}

@incollection{gschwandtner_blensor_2011-1,
	address = {Berlin, Heidelberg},
	title = {{BlenSor}: {Blender} {Sensor} {Simulation} {Toolbox}},
	volume = {6939},
	isbn = {978-3-642-24030-0 978-3-642-24031-7},
	shorttitle = {{BlenSor}},
	url = {http://link.springer.com/10.1007/978-3-642-24031-7_20},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Wang, Song and Kyungnam, Kim and Benes, Bedrich and Moreland, Kenneth and Borst, Christoph and DiVerdi, Stephen and Yi-Jen, Chiang and Ming, Jiang},
	year = {2011},
	doi = {10.1007/978-3-642-24031-7_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {199--208},
}

@inproceedings{farrell_sensor_2008-2,
	address = {San Jose, CA},
	title = {Sensor calibration and simulation},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.767901},
	doi = {10.1117/12.767901},
	urldate = {2024-01-15},
	author = {Farrell, Joyce and Okincha, Michael and Parmar, Manu},
	editor = {DiCarlo, Jeffrey M. and Rodricks, Brian G.},
	month = feb,
	year = {2008},
	pages = {68170R},
}

@misc{heiden_physics-based_2020-2,
	title = {Physics-based {Simulation} of {Continuous}-{Wave} {LIDAR} for {Localization}, {Calibration} and {Tracking}},
	url = {http://arxiv.org/abs/1912.01652},
	abstract = {Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Heiden, Eric and Liu, Ziang and Ramachandran, Ragesh K. and Sukhatme, Gaurav S.},
	month = mar,
	year = {2020},
	note = {Issue: arXiv:1912.01652
arXiv:1912.01652 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: Published at ICRA 2020},
}

@inproceedings{nakajima_lidar_2021-2,
	title = {{LiDAR} {Measurement} {Simulator} {Considering} {Target} {Surface} {Reflection}},
	volume = {8},
	publisher = {ESA Space Debris Office},
	author = {Nakajima, Yu and Sasaki, Takahiro and Okada, Naoki and Yamamoto, Toru},
	year = {2021},
}

@article{lee_validation_2020-2,
	title = {{VALIDATION} {OF} {LIDAR} {CALIBRATION} {USING} {A} {LIDAR} {SIMULATOR}},
	volume = {XLIII-B1-2020},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLIII-B1-2020/39/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-39-2020},
	abstract = {Abstract. LIDAR is being widely used for mapping and modelling because it accurately scans and acquires 3D geometric information of the surrounding environment. In order to improve the accuracy of the LIDAR measurement, it is necessary to precisely estimate the intrinsic parameters as well as extrinsic parameters and eliminate the systematic errors. Many studies are conducted to eliminate these errors caused by the intrinsic parameters of LIDAR. However, when the result of intrinsic calibration is verified using actual LIDAR data, there is a problem that other error factors cannot be excluded. Therefore, in this study, the LIDAR intrinsic calibration is verified by using a LIDAR simulator that simulates the mechanism of the actual LIDAR. When constructing a LIDAR simulator, the systematic error is inserted according to the intrinsic parameter model of LIDAR. And according to the method of scanning with LIDAR, it is divided into upright scanning and tilted scanning, and the error included LIDAR simulation data is generated. After that, the intrinsic parameters are estimated by applying the plane-based intrinsic calibration. Since values of the intrinsic parameters are known, they are compared with the estimated parameters, and the results of estimate are analyzed according to the scanning method.},
	language = {en},
	urldate = {2024-01-15},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lee, G. and Cheon, J. and Lee, I.},
	month = aug,
	year = {2020},
	pages = {39--44},
}

@article{wang_measurement_2013-2,
	title = {Measurement and modeling of {Bidirectional} {Reflectance} {Distribution} {Function} ({BRDF}) on material surface},
	volume = {46},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224113003072},
	doi = {10.1016/j.measurement.2013.07.008},
	language = {en},
	number = {9},
	urldate = {2024-01-15},
	journal = {Measurement},
	author = {Wang, Hongyuan and Zhang, Wei and Dong, Aotuo},
	month = nov,
	year = {2013},
	note = {Number: 9},
	pages = {3654--3661},
}

@article{jutzi_range_2006-2,
	title = {Range determination with waveform recording laser systems using a {Wiener} {Filter}},
	volume = {61},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271606001080},
	doi = {10.1016/j.isprsjprs.2006.09.001},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jutzi, Boris and Stilla, Uwe},
	month = nov,
	year = {2006},
	note = {Number: 2},
	pages = {95--107},
}

@article{gastellu-etchegorry_discrete_2015-2,
	title = {Discrete {Anisotropic} {Radiative} {Transfer} ({DART} 5) for {Modeling} {Airborne} and {Satellite} {Spectroradiometer} and {LIDAR} {Acquisitions} of {Natural} and {Urban} {Landscapes}},
	volume = {7},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/7/2/1667},
	doi = {10.3390/rs70201667},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {Remote Sensing},
	author = {Gastellu-Etchegorry, Jean-Philippe and Yin, Tiangang and Lauret, Nicolas and Cajgfinger, Thomas and Gregoire, Tristan and Grau, Eloi and Feret, Jean-Baptiste and Lopes, Maïlys and Guilleux, Jordan and Dedieu, Gérard and Malenovský, Zbyněk and Cook, Bruce and Morton, Douglas and Rubio, Jeremy and Durrieu, Sylvie and Cazanave, Gregory and Martin, Emmanuel and Ristorcelli, Thomas},
	month = feb,
	year = {2015},
	note = {Number: 2},
	pages = {1667--1701},
}

@misc{zhang_nerf-lidar_2024-2,
	title = {{NeRF}-{LiDAR}: {Generating} {Realistic} {LiDAR} {Point} {Clouds} with {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF}-{LiDAR}},
	url = {http://arxiv.org/abs/2304.14811},
	abstract = {Labeling LiDAR point clouds for training autonomous driving is extremely expensive and difficult. LiDAR simulation aims at generating realistic LiDAR data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages real-world information to generate realistic LIDAR point clouds. Different from existing LiDAR simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our NeRF-LiDAR by training different 3D segmentation models on the generated LiDAR point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real LiDAR data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	month = jan,
	year = {2024},
	note = {Issue: arXiv:2304.14811
arXiv:2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{abhijeet__tallavajhula_lidar_2018-2,
	title = {Lidar {Simulation} for {Robotic} {Application} {Development}: {Modeling} and {Evaluation}},
	copyright = {In Copyright},
	shorttitle = {Lidar {Simulation} for {Robotic} {Application} {Development}},
	url = {https://kilthub.cmu.edu/articles/Lidar_Simulation_for_Robotic_Application_Development_Modeling_and_Evaluation/6720428/1},
	doi = {10.1184/R1/6720428.V1},
	abstract = {Given the increase in scale and complexity of robotics, robot application development is challenging in the real world. It may be expensive, unsafe, or impractical to collect data, or test systems, in reality. Simulation provides an answer to these challenges. In simulation, data collection is relatively inexpensive, scenes can be procedurally generated, and state information is trivially available. Despite these benefits, the use of simulators is often limited to the early stages of application development. In this work, we take steps to close the gap between simulation and reality, for Lidar simulation. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with a scene. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. We formalize the intuition that application performance must be similar in simulation and reality, using an application-level loss. Our framework is broadly applicable to simulating sensors other than Lidars. We instantiate our framework for two domains. The first domain is planar Lidar simulation in indoor scenes. We construct a high-fidelity simulator using a parametric sensor model. We show how application development paths for our simulator are closer to reality, compared to a baseline. We also pose sensor modeling as a case of distribution regression, which leads to a novel application of a nonparametric method that adapts to trends in sensor data. The second domain is Lidar simulation in o -road scenes. Our approach is to build a library of terrain primitives, derived from real Lidar observations. These are shown to generalize, resulting in an expressive simulator for complex o -road scenes. For this domain as well, we quantitatively demonstrate that our simulator is better for application development, compared to a baseline. Our work suggests a generic approach to building useful simulators. We view them as predictive models, and perform thorough tests on real data. We evaluate them with an application-level loss, which supports their greater use in the development cycle.},
	urldate = {2024-01-15},
	author = {{Abhijeet  Tallavajhula}},
	year = {2018},
	note = {Artwork Size: 14724730 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80101 Adaptive Agents and Intelligent Robotics, FOS: Computer and information sciences},
	pages = {14724730 Bytes},
}

@article{wang_dart-lux_2022-2,
	title = {{DART}-{Lux}: {An} unbiased and rapid {Monte} {Carlo} radiative transfer method for simulating remote sensing images},
	volume = {274},
	issn = {00344257},
	shorttitle = {{DART}-{Lux}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425722000876},
	doi = {10.1016/j.rse.2022.112973},
	language = {en},
	urldate = {2024-01-15},
	journal = {Remote Sensing of Environment},
	author = {Wang, Yingjie and Kallel, Abdelaziz and Yang, Xuebo and Regaieg, Omar and Lauret, Nicolas and Guilleux, Jordan and Chavanon, Eric and Gastellu-Etchegorry, Jean-Philippe},
	month = jun,
	year = {2022},
	pages = {112973},
}

@misc{guillard_learning_2022-2,
	title = {Learning to {Simulate} {Realistic} {LiDARs}},
	url = {http://arxiv.org/abs/2209.10986},
	abstract = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or per-point intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene's appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Guillard, Benoit and Vemprala, Sai and Gupta, Jayesh K. and Miksik, Ondrej and Vineet, Vibhav and Fua, Pascal and Kapoor, Ashish},
	month = sep,
	year = {2022},
	note = {Issue: arXiv:2209.10986
arXiv:2209.10986 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	annote = {Comment: IROS2022 paper},
}

@article{lopez_gpu-accelerated_2022-2,
	title = {A {GPU}-{Accelerated} {Framework} for {Simulating} {LiDAR} {Scanning}},
	volume = {60},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9751040/},
	doi = {10.1109/TGRS.2022.3165746},
	urldate = {2024-01-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lopez, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	year = {2022},
	pages = {1--18},
}

@misc{winiwarter_virtual_2021-2,
	title = {Virtual laser scanning with {HELIOS}++: {A} novel take on ray tracing-based simulation of topographic {3D} laser scanning},
	shorttitle = {Virtual laser scanning with {HELIOS}++},
	url = {http://arxiv.org/abs/2101.09154},
	abstract = {Topographic laser scanning is a remote sensing method to create detailed 3D point cloud representations of the Earth's surface. Since data acquisition is expensive, simulations can complement real data given certain premises are available: i) a model of 3D scene and scanner, ii) a model of the beam-scene interaction, simplified to a computationally feasible while physically realistic level, and iii) an application for which simulated data is fit for use. A number of laser scanning simulators for different purposes exist, which we enrich by presenting HELIOS++. HELIOS++ is an open-source simulation framework for terrestrial static, mobile, UAV-based and airborne laser scanning implemented in C++. The HELIOS++ concept provides a flexible solution for the trade-off between physical accuracy (realism) and computational complexity (runtime, memory footprint), as well as ease of use and of configuration. Unique features of HELIOS++ include the availability of Python bindings (pyhelios) for controlling simulations, and a range of model types for 3D scene representation. HELIOS++ further allows the simulation of beam divergence using a subsampling strategy, and is able to create full-waveform outputs as a basis for detailed analysis. As generation and analysis of waveforms can strongly impact runtimes, the user may set the level of detail for the subsampling, or optionally disable full-waveform output altogether. A detailed assessment of computational considerations and a comparison of HELIOS++ to its predecessor, HELIOS, reveal reduced runtimes by up to 83 \%. At the same time, memory requirements are reduced by up to 94 \%, allowing for much larger (i.e. more complex) 3D scenes to be loaded into memory and hence to be virtually acquired by laser scanning simulation.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Winiwarter, Lukas and Pena, Alberto Manuel Esmorís and Weiser, Hannah and Anders, Katharina and Sanchez, Jorge Martínez and Searle, Mark and Höfle, Bernhard},
	month = jan,
	year = {2021},
	note = {Issue: arXiv:2101.09154
arXiv:2101.09154 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{kim_simulation_2013-2,
	title = {Simulation of a {Geiger}-{Mode} {Imaging} {LADAR} {System} for {Performance} {Assessment}},
	volume = {13},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/13/7/8461},
	doi = {10.3390/s130708461},
	language = {en},
	number = {7},
	urldate = {2024-01-15},
	journal = {Sensors},
	author = {Kim, Seongjoon and Lee, Impyeong and Kwon, Yong},
	month = jul,
	year = {2013},
	note = {Number: 7},
	pages = {8461--8489},
}

@misc{manivasagam_lidarsim_2020-2,
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	shorttitle = {{LiDARsim}},
	url = {http://arxiv.org/abs/2006.09348},
	abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	note = {Issue: arXiv:2006.09348
arXiv:2006.09348 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020 (Oral)},
}

@misc{noauthor_velodyne_2019,
	title = {Velodyne {Lidar} {ULTRA} {Puck}},
	url = {https://velodynelidar.com/wp-content/uploads/2019/12/63-9378_Rev-F_Ultra-Puck_Datasheet_Web.pdf},
	publisher = {Velodyne Lidar, Inc},
	year = {2019},
}

@book{patricio_gonzalez_vivo_book_nodate,
	title = {The {Book} of {Shaders}},
	url = {https://thebookofshaders.com/},
	author = {{Patricio Gonzalez Vivo} and {Jen Lowe}},
}

@book{joey_de_vries_learn_nodate,
	title = {Learn {OpenGL} - {Graphics} {Programming}},
	url = {https://learnopengl.com},
	author = {{Joey de Vries}},
}

@book{vries_learn_2020,
	address = {Erscheinungsort nicht ermittelbar},
	title = {Learn {OpenGL} - {Graphics} programming: {Learn} modern {OpenGL} graphics programming in a step-by-step fashion},
	isbn = {978-90-90-33256-7},
	shorttitle = {Learn {OpenGL} - {Graphics} programming},
	abstract = {Graphics programmers are often coined the 'wizards' of the game industry. As every magician knows, terms like wizardry and magic are often somewhat exaggerated. Yet, there is a certain charm to graphics programming: the ability to conjure up complete living worlds at our fingertips. Learn OpenGL will teach you the basics, the intermediate, and tons of advanced knowledge, using modern (core-profile) OpenGL. The aim of this book is to show you all there is to modern OpenGL in an easy-to-understand fashion, with clear examples and step-by-step instructions, while also providing a useful reference for later studies. After years of continuous work and improvements on the accompanying website learnopengl.com, with the help of thousands of readers, its content has been professionally revised for this physical copy you now find in your hands. Graphics programming isn't as hard as many people make it out to be... you just need to start},
	language = {eng},
	publisher = {Kendall \& Welling},
	author = {Vries, Joey de},
	year = {2020},
	annote = {Includes index},
}

@misc{noauthor_carla_nodate,
	title = {{CARLA} {Simulator}},
	url = {https://carla.org//},
}

@misc{mattia_previtali_evaluation_nodate,
	title = {Evaluation of the {Expected} {Data} {Quality} in {Laser} {Scanning} {Surveying} of {Archaeological} {Sites}},
	url = {https://api.semanticscholar.org/CorpusID:231830546},
	author = {{Mattia Previtali} and {Lucia Díaz-Vilariño} and {Marco Scaioni} and {Ernesto Frías Nores}},
}

@article{risbol_lidar_2018,
	title = {{LiDAR} from drones employed for mapping archaeology – {Potential}, benefits and challenges},
	volume = {25},
	issn = {1075-2196, 1099-0763},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/arp.1712},
	doi = {10.1002/arp.1712},
	abstract = {Abstract
            Although the use of both drones and LiDAR (light detection and ranging) has become common in archaeology in recent years, LiDAR scanning from drones is still in its infancy. The technological development related to drones as well as laser scanner instruments has gradually reached the point where these can be integrated. In this paper we present the results from a test where the applicability of LiDAR used from a drone was studied. The study had two objectives – both based on comparative studies: (i) whether LiDAR from drones represents an improvement in terms of detection success; and (ii) whether LiDAR from drones can increase the quality of the documentation of archaeological features and their physical properties based on remote sensing. A modest improvement of detection success was found, but was not as convincing as one would perhaps expect given the relatively large increase in terms of ground points. This has led us to the conclusion that very dense vegetation obstructs laser beams from reaching all the way to the bare earth. As regards accuracy in documenting archaeological features, the study showed more significant improvements. The last part of the paper is dedicated to a discussion of the pros and cons of using LiDAR from drones compared to conventional airborne laser scanning from aeroplanes or helicopters. The main advantages concern flexibility, low flight altitude and small laser footprint as well as the advantages of a far‐reaching field of view. The disadvantages are related to price, battery capacity, size of area and especially the requirement of line of sight between the drone operator and the drone, a fact that restricts the efficiency in terms of mapping large areas. Nevertheless, the final conclusion is that LiDAR from drones has the potential to make a substantial improvement to archaeological remote sensing.},
	language = {en},
	number = {4},
	urldate = {2024-03-19},
	journal = {Archaeological Prospection},
	author = {Risbøl, Ole and Gustavsen, Lars},
	month = oct,
	year = {2018},
	pages = {329--338},
	file = {Submitted Version:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\PXXCAEP9\\Risbøl and Gustavsen - 2018 - LiDAR from drones employed for mapping archaeology.pdf:application/pdf},
}

@article{li_lidar_2020,
	title = {Lidar for {Autonomous} {Driving}: {The} {Principles}, {Challenges}, and {Trends} for {Automotive} {Lidar} and {Perception} {Systems}},
	volume = {37},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Lidar for {Autonomous} {Driving}},
	url = {https://ieeexplore.ieee.org/document/9127855/},
	doi = {10.1109/MSP.2020.2973615},
	number = {4},
	urldate = {2024-03-19},
	journal = {IEEE Signal Processing Magazine},
	author = {Li, You and Ibanez-Guzman, Javier},
	month = jul,
	year = {2020},
	pages = {50--61},
	file = {Submitted Version:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\T77HZU7S\\Li and Ibanez-Guzman - 2020 - Lidar for Autonomous Driving The Principles, Chal.pdf:application/pdf},
}

@misc{leah_a_wasser_basics_nodate,
	title = {The {Basics} of {LiDAR} - {Light} {Detection} and {Ranging} - {Remote} {Sensing}},
	url = {https://www.neonscience.org/resources/learning-hub/tutorials/lidar-basics},
	author = {{Leah A. Wasser}},
}

@article{jutzi_range_2006-3,
	title = {Range determination with waveform recording laser systems using a {Wiener} {Filter}},
	volume = {61},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271606001080},
	doi = {10.1016/j.isprsjprs.2006.09.001},
	language = {en},
	number = {2},
	urldate = {2024-03-19},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jutzi, Boris and Stilla, Uwe},
	month = nov,
	year = {2006},
	pages = {95--107},
}

@misc{noauthor_velodyne_nodate,
	title = {Velodyne {Lidar} {Puck}},
	url = {https://velodynelidar.com/products/puck/},
}

@misc{noauthor_difference_nodate,
	title = {Difference between {Parametric} and {Non}-{Parametric} {Methods}},
	url = {https://www.geeksforgeeks.org/difference-between-parametric-and-non-parametric-methods/},
}

@misc{noauthor_what_nodate,
	title = {What are {Parametric} and {Non}-parametric {Modeling}},
	url = {https://www.pre-scient.com/knowledge-center/reverse-engineering/parametric-and-non-parametric-modelling/},
}

@misc{georgios_nanos_differences_nodate,
	title = {Differences {Between} a {Parametric} and {Non}-parametric {Model}},
	url = {https://www.baeldung.com/cs/ml-parametric-vs-non-parametric-models},
	journal = {Baeldung CS},
	author = {{Georgios Nanos}},
}

@misc{noauthor_adding_nodate,
	title = {Adding {Synthetic} {Noise}},
	url = {https://www.nmr.mgh.harvard.edu/PMI/toolbox/Documentation/pminoise.xhtml},
}

@misc{noauthor_notitle_nodate-1,
}

@misc{noauthor_compute_nodate,
	title = {Compute {Shader}},
	url = {https://www.khronos.org/opengl/wiki/Compute_Shader},
}

@book{weng_qihao_advances_2017,
	address = {Boca Raton},
	edition = {First paperback edition},
	title = {Advances in environmental remote sensing: sensors, algorithms, and applications},
	isbn = {978-1-138-07291-6},
	shorttitle = {Advances in environmental remote sensing},
	abstract = {"Featuring contributors from 17 countries, this book systematically presents recent advances in environmental remote sensing. First, the text provides the latest developments in remote sensing data and sensor systems, paying special attention to LIDAR, high spatial-resolution sensing, hyperspectral sensing, and data fusion of various sensors. Next, authors examine cutting-edge algorithms and techniques for the detection, interpretation, characterization, and modeling of Earth surface features. The final section explores applications of current data, sensors, algorithms, and techniques to the remote sensing of land, vegetation, water, and air."--Provided by publisher},
	language = {eng},
	publisher = {CRC Press},
	author = {{Weng, Qihao}},
	year = {2017},
	note = {OCLC: 982649332},
}

@misc{noauthor_notitle_nodate-2,
}

@misc{noauthor_lidar_nodate-1,
	title = {Lidar},
	url = {https://www.neonscience.org/data-collection/lidar},
	journal = {Airborne Remote Sensing},
}

@misc{noauthor_shader_nodate,
	title = {Shader},
	url = {https://www.khronos.org/opengl/wiki/Shader},
}

@misc{noauthor_what_nodate-1,
	title = {What is {LiDAR} and {How} {Does} {It} {Work}?},
	url = {https://www.jouav.com/blog/what-is-lidar.html},
}

@article{reitmann_blainderblender_2021,
	title = {{BLAINDER}—{A} {Blender} {AI} {Add}-{On} for {Generation} of {Semantically} {Labeled} {Depth}-{Sensing} {Data}},
	volume = {21},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/6/2144},
	doi = {10.3390/s21062144},
	abstract = {Common Machine-Learning (ML) approaches for scene classification require a large amount of training data. However, for classification of depth sensor data, in contrast to image data, relatively few databases are publicly available and manual generation of semantically labeled 3D point clouds is an even more time-consuming task. To simplify the training data generation process for a wide range of domains, we have developed the BLAINDER add-on package for the open-source 3D modeling software Blender, which enables a largely automated generation of semantically annotated point-cloud data in virtual 3D environments. In this paper, we focus on classical depth-sensing techniques Light Detection and Ranging (LiDAR) and Sound Navigation and Ranging (Sonar). Within the BLAINDER add-on, different depth sensors can be loaded from presets, customized sensors can be implemented and different environmental conditions (e.g., influence of rain, dust) can be simulated. The semantically labeled data can be exported to various 2D and 3D formats and are thus optimized for different ML applications and visualizations. In addition, semantically labeled images can be exported using the rendering functionalities of Blender.},
	language = {en},
	number = {6},
	urldate = {2024-03-25},
	journal = {Sensors},
	author = {Reitmann, Stefan and Neumann, Lorenzo and Jung, Bernhard},
	month = mar,
	year = {2021},
	pages = {2144},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\SWCUC2FG\\Reitmann et al. - 2021 - BLAINDER—A Blender AI Add-On for Generation of Sem.pdf:application/pdf},
}

@misc{jarlballin89_maple_nodate,
	title = {Maple {Tree}},
	url = {https://sketchfab.com/3d-models/maple-tree-68bea58fd9a549a99cfa5d1c739c97a8},
	author = {{JarlBallin89}},
}

@misc{krzysztof_stolorz_1929_nodate,
	title = {1929 {BMW} 3/15 ({Dixi}, {LP})},
	url = {https://sketchfab.com/3d-models/1929-bmw-315-dixi-lp-df5748546cdb429ba3c5038697e4a4d4},
	author = {{Krzysztof Stolorz}},
}

@misc{drcg_building_nodate,
	title = {Building {No} 6 form {Tokyo} {Otemachi} {Building} {Pack}},
	url = {https://sketchfab.com/3d-models/building-no-6-form-tokyo-otemachi-building-pack-e8e34afb92ba480f91af6f67a0471015},
	author = {{DrCG}},
}

@misc{joey_de_vries_getting_nodate,
	title = {Getting {Started}: {Shaders}},
	url = {https://learnopengl.com/Getting-started/Shaders},
	author = {{Joey de Vries}},
}

@misc{gavri_kepets_boids_nodate,
	title = {Boids and {Compute} {Shaders}},
	url = {https://gkgkgkgk.github.io/shaders/2022/01/19/boids.html},
	author = {{Gavri Kepets}},
}

@article{va_real-time_2021,
	title = {Real-{Time} {Cloth} {Simulation} {Using} {Compute} {Shader} in {Unity3D} for {AR}/{VR} {Contents}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/17/8255},
	doi = {10.3390/app11178255},
	abstract = {While the cloth component in Unity engine has been used to represent the 3D cloth object for augmented reality (AR) and virtual reality (VR), it has several limitations in term of resolution and performance. The purpose of our research is to develop a stable cloth simulation based on a parallel algorithm. The method of a mass–spring system is applied to real-time cloth simulation with three types of springs. However, cloth simulation using the mass–spring system requires a small integration time-step to use a large stiffness coefficient. Furthermore, constraint enforcement is applied to obtain the stable behavior of the cloth model. To reduce the computational burden of constraint enforcement, the adaptive constraint activation and deactivation (ACAD) technique that includes the mass–spring system and constraint enforcement method is applied to prevent excessive elongation of the cloth. The proposed algorithm utilizes the graphics processing unit (GPU) parallel processing, and implements it in Compute Shader that executes in different pipelines to the rendering pipeline. In this paper, we investigate the performance and compare the behavior of the mass–spring system, constraint enforcement, and ACAD techniques using a GPU-based parallel method.},
	language = {en},
	number = {17},
	urldate = {2024-03-26},
	journal = {Applied Sciences},
	author = {Va, Hongly and Choi, Min-Hyung and Hong, Min},
	month = sep,
	year = {2021},
	pages = {8255},
	file = {Full Text:C\:\\Users\\Gavri Kepets\\Zotero\\storage\\JUSFEYFF\\Va et al. - 2021 - Real-Time Cloth Simulation Using Compute Shader in.pdf:application/pdf},
}

@inproceedings{junker_real-time_2020,
	address = {Bugibba Malta},
	title = {Real-time {Interactive} {Snow} {Simulation} using {Compute} {Shaders} in {Digital} {Environments}},
	isbn = {978-1-4503-8807-8},
	url = {https://dl.acm.org/doi/10.1145/3402942.3402995},
	doi = {10.1145/3402942.3402995},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {International {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {ACM},
	author = {Junker, Andreas and Palamas, George},
	month = sep,
	year = {2020},
	pages = {1--4},
}

@misc{brian_caulfield_whats_nodate,
	title = {What’s the {Difference} {Between} {Ray} {Tracing} and {Rasterization}?},
	url = {https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing-rasterization/},
	author = {{Brian Caulfield}},
}

@book{noauthor_ray_2021,
	title = {Ray {Tracing} {Gems}},
	isbn = {978-1-4842-4426-5},
	language = {eng},
	publisher = {Springer Nature},
	year = {2021},
	note = {OCLC: 1328776805},
}

@misc{henrik_ray_nodate,
	title = {Ray trace diagram},
	url = {https://commons.wikimedia.org/wiki/File:Ray_trace_diagram.svg},
	author = {{Henrik}},
}

@book{pharr_physically_2017,
	address = {Cambridge, MA},
	edition = {Third edition},
	title = {Physically based rendering: from theory to implementation},
	isbn = {978-0-12-800645-0},
	shorttitle = {Physically based rendering},
	publisher = {Morgan Kaufmann Publishers/Elsevier},
	author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
	year = {2017},
	note = {OCLC: ocn936532273},
	keywords = {Computer graphics, Digital techniques, Dreidimensionale Computergrafik, Image processing, Realistische Computergrafik, Rendering, Three-dimensional display systems},
	annote = {Index of notation on lining papers},
	annote = {Introduction -- Geometry and transformations -- Shapes -- Primitives and intersection acceleration -- Color radiometry -- Camera models -- Sampling and reconstruction -- Reflection models -- Materials -- Texture -- Volume scattering -- Light sources -- Monte Carlo integration -- Light transport I : surface reflection -- Light transport II : volume rendering -- Light transport III : bidirectional methods -- Retrospective and the future},
}

@misc{noauthor_lambertian_nodate,
	title = {Lambertian reflectance},
	url = {https://en.wikipedia.org/wiki/Lambertian_reflectance},
}

@book{ian_dunn_graphics_nodate,
	title = {Graphics {Programming} {Compendium}},
	url = {https://graphicscompendium.com/index.html},
	author = {{Ian Dunn} and {Zoe Wood}},
}

@article{howard_ureteral_1951,
	title = {The ureteral splint in the repair of ureteropelvic avulsion},
	volume = {18},
	language = {eng},
	journal = {Transactions. American Urological Association. Western Section},
	author = {Howard, F. S. and Hinman, F.},
	year = {1951},
	pmid = {14922512},
	keywords = {Fractures, Bone, Humans, Splints, Stents, Ureter, URETERS/surgery, Wound Healing},
	pages = {142--151},
}

@article{brian_karis_real_nodate,
	title = {Real {Shading} in {Unreal} {Engine} 4},
	url = {https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf},
	author = {{Brian Karis}},
}

@book{kautz_rendering_2007,
	address = {Aire-la-Ville},
	title = {Rendering techniques 2007: [18th] {Eurographics} {Symposium} on {Rendering} ; {Grenoble}, {France}, {June} 25-27, 2007},
	isbn = {978-3-905673-52-4 978-1-56881-364-6},
	shorttitle = {Rendering techniques 2007},
	language = {eng},
	publisher = {Eurographics Association [u.a.]},
	editor = {Kautz, Jan and Pattanaik, Sumanta and Holzschuh, Nicolas and {European Association for Computer Graphics}},
	year = {2007},
	note = {Meeting Name: Symposium on Rendering},
	annote = {Literaturangaben},
}

@article{noauthor_microfacet_nodate,
	title = {Microfacet models for refraction through rough surfaces},
}

@article{walter_microfacet_2007,
	title = {Microfacet {Models} for {Refraction} through {Rough} {Surfaces}},
	issn = {1727-3463},
	url = {http://diglib.eg.org/handle/10.2312/EGWR.EGSR07.195-206},
	doi = {10.2312/EGWR/EGSR07/195-206},
	abstract = {Microfacet models have proven very successful for modeling light reflection from rough surfaces. In this paper we review microfacet theory and demonstrate how it can be extended to simulate transmission through rough surfaces such as etched glass. We compare the resulting transmission model to measured data from several real surfaces and discuss appropriate choices for the microfacet distribution and shadowing-masking functions. Since rendering transmission through media requires tracking light that crosses at least two interfaces, good importance sampling is a practical necessity. Therefore, we also describe efficient schemes for sampling the microfacet models and the corresponding probability density functions.},
	language = {en},
	urldate = {2024-03-26},
	journal = {Rendering Techniques},
	author = {Walter, Bruce and Marschner, Stephen R. and Li, Hongsong and Torrance, Kenneth E.},
	year = {2007},
	note = {Artwork Size: 12 pages
ISBN: 9783905673524
Publisher: [object Object]},
	keywords = {Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Three-Dimensional Graphics and Realism]:},
	pages = {12 pages},
	annote = {SeriesInformation
Rendering Techniques},
}

@misc{slamtec_rplidar_nodate,
	title = {{RPLIDAR} {A1}},
	url = {https://www.slamtec.ai/product/slamtec-rplidar-a1/},
	author = {{Slamtec}},
}
